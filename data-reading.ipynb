{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "import PyPDF2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('teste.pdf', 'rb') as f:\n",
    "    pdf = PyPDF2.PdfFileReader(f)\n",
    "    print(pdf.getPage(2).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-854578e4712f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/MBA FGV/Análise de Mídias Sociais e Mineração de Textos/python-text-mining/venv-text/lib/python3.8/site-packages/PyPDF2/pdf.py\u001b[0m in \u001b[0;36mgetPage\u001b[0;34m(self, pageNumber)\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflattenedPages\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflattenedPages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpageNumber\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m     namedDestinations = property(lambda self:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pdf.getPage(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl 'http://www.planalto.gov.br/ccivil_03/leis/l6404consol.htm' \\\n",
    "  -H 'Connection: keep-alive' \\\n",
    "  -H 'Cache-Control: max-age=0' \\\n",
    "  -H 'Upgrade-Insecure-Requests: 1' \\\n",
    "  -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36' \\\n",
    "  -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9' \\\n",
    "  -H 'Referer: http://www.planalto.gov.br/ccivil_03/leis/l6404consol.htm' \\\n",
    "  -H 'Accept-Language: pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7' \\\n",
    "  -H 'Cookie: f5_cspm=1234; f5avr0155180568aaaaaaaaaaaaaaaa=LLFPPKCFMGINLBGOEKKGGPOGMNLNFABIPDIBPJBNMCLCONEMANAMCMKGCIILCIJIKJJIELNDMFFCAAJFCCPJDNPNDIJAMHCMBHDMGEKDELDNHFOABAJHABODBKGFJPGB; TS01bca3bd=0150f80db109a1cdb20a071c1e09f9e790dca05aca8cdc0133eb6e9f80ae8eb56bf50118ea76748749687de2b1e809d459edf092a84176dfdd4dedbf076b7a0b6921ccf2a8' \\\n",
    "  -H 'If-None-Match: \"08b6b863967d61:0\"' \\\n",
    "  -H 'If-Modified-Since: Fri, 31 Jul 2020 12:53:02 GMT' \\\n",
    "  --compressed \\\n",
    "  --insecure > lei.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.globo.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = html.fromstring(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "content =h.findall('*//a') + h.findall('*//p')\n",
    "\n",
    "for i in range(1,6):\n",
    "    content += h.findall(f'*//h{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [i.text.strip() for i in content if i.text is not None and len(i.text.strip()) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Política',\n",
       " 'Supremo Tribunal Federal',\n",
       " 'preso desde 2016',\n",
       " 'Fux assumiu a presidência na última quinta',\n",
       " 'homologou a colaboração premiada assinada pelo político',\n",
       " 'Polícia Federal',\n",
       " 'Procuradoria-Geral da República',\n",
       " 'recorreu da decisão de Fachin que validou a delação',\n",
       " 'Sérgio Cabral',\n",
       " 'Dias Toffoli',\n",
       " 'Luiz Edson Fachin',\n",
       " 'STF - Supremo Tribunal Federal',\n",
       " 'Inscreva-se e receba a newsletter',\n",
       " 'Toffoli arquiva todos os inquéritos abertos no STF a partir da delação de Cabral',\n",
       " \"Secretário de Fazenda fica em situação delicada após 'cartão vermelho' de Bolsonaro\",\n",
       " \"Guedes lamenta 'barulheira' e diz que propostas ainda estavam em estudo\",\n",
       " 'JULIA DUAILIBI: ala econômica insiste em ideias politicamente inviáveis',\n",
       " 'Brasil registra 1.090 mortes por Covid em 24 h e passa de 133 mil',\n",
       " 'Taxa de contágio no Brasil é a menor desde abril, aponta estudo',\n",
       " 'Pesquisa mostra desaceleração no país, mas aumento de casos entre idosos e crianças',\n",
       " 'Teste da vacina de Oxford no Brasil vai dobrar nº de voluntários',\n",
       " 'Confira as dezenas da Mega-Sena',\n",
       " 'Fux muda orientação do CNJ sobre benefício para presos na pandemia',\n",
       " 'Estados com melhor Ideb têm ensino integral e jovens protagonistas',\n",
       " 'Ensino médio evolui, mas país segue longe da meta',\n",
       " 'Alertas de desmatamento crescem 68% na Amazônia em agosto, diz Imazon',\n",
       " 'Veja mais',\n",
       " 'Últimas Notícias',\n",
       " 'princípios editoriais',\n",
       " 'política de privacidade',\n",
       " 'minha conta',\n",
       " 'anuncie conosco',\n",
       " 'Por Márcio Falcão e Fernanda Vivas, TV Globo',\n",
       " 'O ministro Dias Toffoli, do',\n",
       " 'A decisão foi tomada pelo ministro antes de deixar a presidência do tribunal — Luiz',\n",
       " 'Essas investigações envolviam, por exemplo, ministros do Tribunal de Contas da União (TCU) e do Superior Tribunal de Justiça (STJ) que teriam sido implicados pelo ex-governador em seus depoimentos.',\n",
       " 'Toffoli atendeu a um pedido da',\n",
       " 'O procurador-geral da República, Augusto Aras,',\n",
       " 'De acordo com informações do STF, o presidente da Corte tem competência para decidir sobre ações ou recursos ineptos ou de outro modo manifestamente inadmissíveis, inclusive por incompetência.',\n",
       " 'O tribunal apontou ainda que “havendo manifestação do PGR, titular da ação penal, pelo arquivamento do inquérito, por entender manifestamente inadmissível a abertura de novo procedimento investigatório na Corte, nada impede que o presidente opere o arquivamento do feito”, como prevê o regimento.',\n",
       " 'A TV Globo apurou que advogados de',\n",
       " 'Procurada, a defesa do ex-governador afirmou que não se manifesta sobre processos sob sigilo.',\n",
       " 'STF arquivou parte dos inquèritos que surgiram a partir de delação de Sérgio Cabral',\n",
       " 'O que aconteceu hoje, diretamente no seu e-mail',\n",
       " 'Obrigado!',\n",
       " 'Você acaba de se inscrever na newsletter Resumo do dia.',\n",
       " 'Veja também',\n",
       " 'Toffoli arquiva todos os inquéritos abertos no STF a partir da delação de Sérgio Cabral',\n",
       " 'Decisão atinge 12 novas frentes de investigação autorizadas pelo ministro Edson Fachin com base na colaboração premiada firmada pelo ex-governador do Rio com a Polícia Federal.']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t#[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_html(url: str, join = True):\n",
    "    h = html.fromstring(\n",
    "        requests.get(url).content\n",
    "        )\n",
    "    \n",
    "    content =h.findall('*//a') + h.findall('*//p')\n",
    "\n",
    "    for i in range(1,6):\n",
    "        \n",
    "        content += h.findall(f'*//h{i}')\n",
    "        \n",
    "    t = [i.text.strip() for i in content if i.text is not None and len(i.text.strip()) > 0]\n",
    "    \n",
    "    if join:\n",
    "        \n",
    "        t = ' '.join(t)\n",
    "        \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = scrap_html('https://blog.hartleybrody.com/web-scraping-cheat-sheet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lei.htm', 'rb') as f:\n",
    "    h = html.fromstring(\n",
    "        f.read()\n",
    "        )\n",
    "\n",
    "content =h.findall('*//a') + h.findall('*//p')\n",
    "\n",
    "for i in range(1,6):\n",
    "\n",
    "    content += h.findall(f'*//h{i}')\n",
    "\n",
    "t = [i.text.strip() for i in content if i.text is not None and len(i.text.strip()) > 0]\n",
    "\n",
    "join = True\n",
    "if join:\n",
    "\n",
    "    t = ' '.join(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hartley',\n",
       " 'Brody',\n",
       " 'my',\n",
       " 'web',\n",
       " 'scraping',\n",
       " 'course',\n",
       " 'Useful',\n",
       " 'Libraries',\n",
       " 'Making',\n",
       " 'Simple',\n",
       " 'Requests',\n",
       " 'Inspecting',\n",
       " 'the',\n",
       " 'Response',\n",
       " 'Extracting',\n",
       " 'Content',\n",
       " 'from',\n",
       " 'HTML',\n",
       " 'Using',\n",
       " 'Regular',\n",
       " 'Expressions',\n",
       " 'Using',\n",
       " 'BeautifulSoup',\n",
       " 'Using',\n",
       " 'XPath',\n",
       " 'Selectors',\n",
       " 'Storing',\n",
       " 'Your',\n",
       " 'Data',\n",
       " 'Writing',\n",
       " 'to',\n",
       " 'a',\n",
       " 'CSV',\n",
       " 'Writing',\n",
       " 'to',\n",
       " 'a',\n",
       " 'SQLite',\n",
       " 'Database',\n",
       " 'More',\n",
       " 'Advanced',\n",
       " 'Topics',\n",
       " 'Javascript',\n",
       " 'Heavy',\n",
       " 'Websites',\n",
       " 'Content',\n",
       " 'Inside',\n",
       " 'iFrames',\n",
       " 'Sessions',\n",
       " 'and',\n",
       " 'Cookies',\n",
       " 'Delays',\n",
       " 'and',\n",
       " 'Backing',\n",
       " 'Off',\n",
       " 'Spoofing',\n",
       " 'the',\n",
       " 'User',\n",
       " 'Agent',\n",
       " 'Using',\n",
       " 'Proxy',\n",
       " 'Servers',\n",
       " 'Setting',\n",
       " 'Timeouts',\n",
       " 'Handling',\n",
       " 'Network',\n",
       " 'Errors',\n",
       " 'Learn',\n",
       " 'More',\n",
       " 'famously',\n",
       " 'NOT',\n",
       " 'recommended',\n",
       " 'at',\n",
       " 'all',\n",
       " 'you',\n",
       " 'can',\n",
       " 'use',\n",
       " 'the',\n",
       " 'lxml',\n",
       " 'library',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'BeautifulSoup',\n",
       " ',',\n",
       " 'as',\n",
       " 'described',\n",
       " 'here',\n",
       " '.',\n",
       " 'use',\n",
       " 'a',\n",
       " 'proxy',\n",
       " 'server',\n",
       " 'ebook',\n",
       " 'online',\n",
       " 'course',\n",
       " 'free',\n",
       " 'sandbox',\n",
       " 'website',\n",
       " 'subscribe',\n",
       " 'to',\n",
       " 'my',\n",
       " 'blog',\n",
       " 'Contact',\n",
       " 'me',\n",
       " 'check',\n",
       " 'out',\n",
       " 'my',\n",
       " 'side',\n",
       " 'projects',\n",
       " 'web',\n",
       " 'scraping',\n",
       " 'content',\n",
       " 'library',\n",
       " 'I',\n",
       " 'Don',\n",
       " '’',\n",
       " 't',\n",
       " 'Need',\n",
       " 'No',\n",
       " 'Stinking',\n",
       " 'API',\n",
       " ':',\n",
       " 'Web',\n",
       " 'Scraping',\n",
       " 'For',\n",
       " 'Fun',\n",
       " 'and',\n",
       " 'Profit',\n",
       " 'Facebook',\n",
       " 'Messenger',\n",
       " 'Bot',\n",
       " 'Tutorial',\n",
       " ':',\n",
       " 'Step-by-Step',\n",
       " 'Instructions',\n",
       " 'for',\n",
       " 'Building',\n",
       " 'a',\n",
       " 'Basic',\n",
       " 'Facebook',\n",
       " 'Chat',\n",
       " 'Bot',\n",
       " 'Web',\n",
       " 'Scraping',\n",
       " 'Reference',\n",
       " ':',\n",
       " 'A',\n",
       " 'Simple',\n",
       " 'Cheat',\n",
       " 'Sheet',\n",
       " 'for',\n",
       " 'Web',\n",
       " 'Scraping',\n",
       " 'with',\n",
       " 'Python',\n",
       " 'Startup',\n",
       " 'Security',\n",
       " 'Guide',\n",
       " ':',\n",
       " 'Minimum',\n",
       " 'Viable',\n",
       " 'Security',\n",
       " 'Checklist',\n",
       " 'for',\n",
       " 'a',\n",
       " 'Cloud-Based',\n",
       " 'Web',\n",
       " 'Application',\n",
       " 'How',\n",
       " 'to',\n",
       " 'Scrape',\n",
       " 'Amazon.com',\n",
       " ':',\n",
       " '19',\n",
       " 'Lessons',\n",
       " 'I',\n",
       " 'Learned',\n",
       " 'While',\n",
       " 'Crawling',\n",
       " '1MM+',\n",
       " 'Product',\n",
       " 'Listings',\n",
       " 'Scaling',\n",
       " 'Your',\n",
       " 'Web',\n",
       " 'App',\n",
       " '101',\n",
       " ':',\n",
       " 'Lessons',\n",
       " 'in',\n",
       " 'Architecture',\n",
       " 'Under',\n",
       " 'Load',\n",
       " 'How',\n",
       " 'HTTPS',\n",
       " 'Secures',\n",
       " 'Connections',\n",
       " ':',\n",
       " 'What',\n",
       " 'Every',\n",
       " 'Web',\n",
       " 'Dev',\n",
       " 'Should',\n",
       " 'Know',\n",
       " 'Peeling',\n",
       " 'Back',\n",
       " 'the',\n",
       " 'ORM',\n",
       " ':',\n",
       " 'Demystifying',\n",
       " 'Relational',\n",
       " 'Databases',\n",
       " 'For',\n",
       " 'New',\n",
       " 'Web',\n",
       " 'Developers',\n",
       " 'Lightning',\n",
       " 'Fast',\n",
       " 'Data',\n",
       " 'Serialization',\n",
       " 'in',\n",
       " 'Python',\n",
       " 'Minimum',\n",
       " 'Viable',\n",
       " 'Git',\n",
       " 'Best',\n",
       " 'Practices',\n",
       " 'for',\n",
       " 'Small',\n",
       " 'Teams',\n",
       " '7',\n",
       " 'Reasons',\n",
       " 'I',\n",
       " 'Wo',\n",
       " \"n't\",\n",
       " 'Sign',\n",
       " 'Your',\n",
       " 'NDA',\n",
       " 'Before',\n",
       " 'a',\n",
       " 'Coffee',\n",
       " 'Meeting',\n",
       " 'Focus',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Product',\n",
       " ',',\n",
       " 'Not',\n",
       " 'the',\n",
       " 'Code',\n",
       " ':',\n",
       " 'How',\n",
       " 'I',\n",
       " 'Build',\n",
       " 'Software',\n",
       " 'for',\n",
       " 'Clients',\n",
       " 'How',\n",
       " 'I',\n",
       " 'Learned',\n",
       " 'to',\n",
       " 'Code',\n",
       " 'in',\n",
       " 'Only',\n",
       " '6',\n",
       " 'Years',\n",
       " ':',\n",
       " 'And',\n",
       " 'You',\n",
       " 'Can',\n",
       " 'Too',\n",
       " '!',\n",
       " 'Web',\n",
       " 'Scraping',\n",
       " 'Content',\n",
       " 'Library',\n",
       " 'Contact',\n",
       " 'Me',\n",
       " 'Press',\n",
       " 'Mentions',\n",
       " 'Recent',\n",
       " 'Projects',\n",
       " 'Subscribe',\n",
       " 'To',\n",
       " 'My',\n",
       " 'Blog',\n",
       " 'Hartley',\n",
       " 'Brody',\n",
       " 'Once',\n",
       " 'you',\n",
       " '’',\n",
       " 've',\n",
       " 'put',\n",
       " 'together',\n",
       " 'enough',\n",
       " 'web',\n",
       " 'scrapers',\n",
       " ',',\n",
       " 'you',\n",
       " 'start',\n",
       " 'to',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'you',\n",
       " 'can',\n",
       " 'do',\n",
       " 'it',\n",
       " 'in',\n",
       " 'your',\n",
       " 'sleep',\n",
       " '.',\n",
       " 'I',\n",
       " '’',\n",
       " 've',\n",
       " 'probably',\n",
       " 'built',\n",
       " 'hundreds',\n",
       " 'of',\n",
       " 'scrapers',\n",
       " 'over',\n",
       " 'the',\n",
       " 'years',\n",
       " 'for',\n",
       " 'my',\n",
       " 'own',\n",
       " 'projects',\n",
       " ',',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'for',\n",
       " 'clients',\n",
       " 'and',\n",
       " 'students',\n",
       " 'in',\n",
       " 'Occasionally',\n",
       " 'though',\n",
       " ',',\n",
       " 'I',\n",
       " 'find',\n",
       " 'myself',\n",
       " 'referencing',\n",
       " 'documentation',\n",
       " 'or',\n",
       " 're-reading',\n",
       " 'old',\n",
       " 'code',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'snippets',\n",
       " 'I',\n",
       " 'can',\n",
       " 'reuse',\n",
       " '.',\n",
       " 'One',\n",
       " 'of',\n",
       " 'the',\n",
       " 'students',\n",
       " 'in',\n",
       " 'my',\n",
       " 'course',\n",
       " 'suggested',\n",
       " 'I',\n",
       " 'put',\n",
       " 'together',\n",
       " 'a',\n",
       " '“',\n",
       " 'cheat',\n",
       " 'sheet',\n",
       " '”',\n",
       " 'of',\n",
       " 'commonly',\n",
       " 'used',\n",
       " 'code',\n",
       " 'snippets',\n",
       " 'and',\n",
       " 'patterns',\n",
       " 'for',\n",
       " 'easy',\n",
       " 'reference',\n",
       " '.',\n",
       " 'I',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'publish',\n",
       " 'it',\n",
       " 'publicly',\n",
       " 'as',\n",
       " 'well',\n",
       " '–',\n",
       " 'as',\n",
       " 'an',\n",
       " 'organized',\n",
       " 'set',\n",
       " 'of',\n",
       " 'easy-to-reference',\n",
       " 'notes',\n",
       " '–',\n",
       " 'in',\n",
       " 'case',\n",
       " 'they',\n",
       " '’',\n",
       " 're',\n",
       " 'helpful',\n",
       " 'to',\n",
       " 'others',\n",
       " '.',\n",
       " 'While',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'written',\n",
       " 'primarily',\n",
       " 'for',\n",
       " 'people',\n",
       " 'who',\n",
       " 'are',\n",
       " 'new',\n",
       " 'to',\n",
       " 'programming',\n",
       " ',',\n",
       " 'I',\n",
       " 'also',\n",
       " 'hope',\n",
       " 'that',\n",
       " 'it',\n",
       " '’',\n",
       " 'll',\n",
       " 'be',\n",
       " 'helpful',\n",
       " 'to',\n",
       " 'those',\n",
       " 'who',\n",
       " 'already',\n",
       " 'have',\n",
       " 'a',\n",
       " 'background',\n",
       " 'in',\n",
       " 'software',\n",
       " 'or',\n",
       " 'python',\n",
       " ',',\n",
       " 'but',\n",
       " 'who',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'some',\n",
       " 'web',\n",
       " 'scraping',\n",
       " 'fundamentals',\n",
       " 'and',\n",
       " 'concepts',\n",
       " '.',\n",
       " 'For',\n",
       " 'the',\n",
       " 'most',\n",
       " 'part',\n",
       " ',',\n",
       " 'a',\n",
       " 'scraping',\n",
       " 'program',\n",
       " 'deals',\n",
       " 'with',\n",
       " 'making',\n",
       " 'HTTP',\n",
       " 'requests',\n",
       " 'and',\n",
       " 'parsing',\n",
       " 'HTML',\n",
       " 'responses',\n",
       " '.',\n",
       " 'I',\n",
       " 'always',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'I',\n",
       " 'have',\n",
       " 'Then',\n",
       " ',',\n",
       " 'at',\n",
       " 'the',\n",
       " 'top',\n",
       " 'of',\n",
       " 'your',\n",
       " 'Make',\n",
       " 'a',\n",
       " 'simple',\n",
       " 'GET',\n",
       " 'request',\n",
       " '(',\n",
       " 'just',\n",
       " 'fetching',\n",
       " 'a',\n",
       " 'page',\n",
       " ')',\n",
       " 'Make',\n",
       " 'a',\n",
       " 'POST',\n",
       " 'requests',\n",
       " '(',\n",
       " 'usually',\n",
       " 'used',\n",
       " 'when',\n",
       " 'sending',\n",
       " 'information',\n",
       " 'to',\n",
       " 'the',\n",
       " 'server',\n",
       " 'like',\n",
       " 'submitting',\n",
       " 'a',\n",
       " 'form',\n",
       " ')',\n",
       " 'Pass',\n",
       " 'query',\n",
       " 'arguments',\n",
       " 'aka',\n",
       " 'URL',\n",
       " 'parameters',\n",
       " '(',\n",
       " 'usually',\n",
       " 'used',\n",
       " 'when',\n",
       " 'making',\n",
       " 'a',\n",
       " 'search',\n",
       " 'query',\n",
       " 'or',\n",
       " 'paging',\n",
       " 'through',\n",
       " 'results',\n",
       " ')',\n",
       " 'See',\n",
       " 'what',\n",
       " 'response',\n",
       " 'code',\n",
       " 'the',\n",
       " 'server',\n",
       " 'sent',\n",
       " 'back',\n",
       " '(',\n",
       " 'useful',\n",
       " 'for',\n",
       " 'detecting',\n",
       " '4XX',\n",
       " 'or',\n",
       " '5XX',\n",
       " 'errors',\n",
       " ')',\n",
       " 'Access',\n",
       " 'the',\n",
       " 'full',\n",
       " 'response',\n",
       " 'as',\n",
       " 'text',\n",
       " '(',\n",
       " 'get',\n",
       " 'the',\n",
       " 'HTML',\n",
       " 'of',\n",
       " 'the',\n",
       " 'page',\n",
       " 'in',\n",
       " 'a',\n",
       " 'big',\n",
       " 'string',\n",
       " ')',\n",
       " 'Look',\n",
       " 'for',\n",
       " 'a',\n",
       " 'specific',\n",
       " 'substring',\n",
       " 'of',\n",
       " 'text',\n",
       " 'within',\n",
       " 'the',\n",
       " 'response',\n",
       " 'Check',\n",
       " 'the',\n",
       " 'response',\n",
       " '’',\n",
       " 's',\n",
       " 'Content',\n",
       " 'Type',\n",
       " '(',\n",
       " 'see',\n",
       " 'if',\n",
       " 'you',\n",
       " 'got',\n",
       " 'back',\n",
       " 'HTML',\n",
       " ',',\n",
       " 'JSON',\n",
       " ',',\n",
       " 'XML',\n",
       " ',',\n",
       " 'etc',\n",
       " ')',\n",
       " 'Now',\n",
       " 'that',\n",
       " 'you',\n",
       " '’',\n",
       " 've',\n",
       " 'made',\n",
       " 'your',\n",
       " 'HTTP',\n",
       " 'request',\n",
       " 'and',\n",
       " 'gotten',\n",
       " 'some',\n",
       " 'HTML',\n",
       " 'content',\n",
       " ',',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'time',\n",
       " 'to',\n",
       " 'parse',\n",
       " 'it',\n",
       " 'so',\n",
       " 'that',\n",
       " 'you',\n",
       " 'can',\n",
       " 'extract',\n",
       " 'the',\n",
       " 'values',\n",
       " 'you',\n",
       " '’',\n",
       " 're',\n",
       " 'looking',\n",
       " 'for',\n",
       " '.',\n",
       " 'Using',\n",
       " 'Regular',\n",
       " 'Expressions',\n",
       " 'to',\n",
       " 'look',\n",
       " 'for',\n",
       " 'HTML',\n",
       " 'patterns',\n",
       " 'is',\n",
       " 'However',\n",
       " ',',\n",
       " 'regular',\n",
       " 'expressions',\n",
       " 'are',\n",
       " 'still',\n",
       " 'useful',\n",
       " 'for',\n",
       " 'finding',\n",
       " 'specific',\n",
       " 'string',\n",
       " 'patterns',\n",
       " 'like',\n",
       " 'prices',\n",
       " ',',\n",
       " 'email',\n",
       " 'addresses',\n",
       " 'or',\n",
       " 'phone',\n",
       " 'numbers',\n",
       " '.',\n",
       " 'Run',\n",
       " 'a',\n",
       " 'regular',\n",
       " 'expression',\n",
       " 'on',\n",
       " 'the',\n",
       " 'response',\n",
       " 'text',\n",
       " 'to',\n",
       " 'look',\n",
       " 'for',\n",
       " 'specific',\n",
       " 'string',\n",
       " 'patterns',\n",
       " ':',\n",
       " 'BeautifulSoup',\n",
       " 'is',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'due',\n",
       " 'to',\n",
       " 'its',\n",
       " 'simple',\n",
       " 'API',\n",
       " 'and',\n",
       " 'its',\n",
       " 'powerful',\n",
       " 'extraction',\n",
       " 'capabilities',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'many',\n",
       " 'different',\n",
       " 'parser',\n",
       " 'options',\n",
       " 'that',\n",
       " 'allow',\n",
       " 'it',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'even',\n",
       " 'the',\n",
       " 'most',\n",
       " 'poorly',\n",
       " 'written',\n",
       " 'HTML',\n",
       " 'pages',\n",
       " '–',\n",
       " 'and',\n",
       " 'the',\n",
       " 'default',\n",
       " 'one',\n",
       " 'works',\n",
       " 'great',\n",
       " '.',\n",
       " 'Compared',\n",
       " 'to',\n",
       " 'libraries',\n",
       " 'that',\n",
       " 'offer',\n",
       " 'similar',\n",
       " 'functionality',\n",
       " ',',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'a',\n",
       " 'pleasure',\n",
       " 'to',\n",
       " 'use',\n",
       " '.',\n",
       " 'To',\n",
       " 'get',\n",
       " 'started',\n",
       " ',',\n",
       " 'you',\n",
       " '’',\n",
       " 'll',\n",
       " 'have',\n",
       " 'to',\n",
       " 'turn',\n",
       " 'the',\n",
       " 'HTML',\n",
       " 'text',\n",
       " 'that',\n",
       " 'you',\n",
       " 'got',\n",
       " 'in',\n",
       " 'the',\n",
       " 'response',\n",
       " 'into',\n",
       " 'a',\n",
       " 'nested',\n",
       " ',',\n",
       " 'DOM-like',\n",
       " 'structure',\n",
       " 'that',\n",
       " 'you',\n",
       " 'can',\n",
       " 'traverse',\n",
       " 'and',\n",
       " 'search',\n",
       " 'Look',\n",
       " 'for',\n",
       " 'all',\n",
       " 'anchor',\n",
       " 'tags',\n",
       " 'on',\n",
       " 'the',\n",
       " 'page',\n",
       " '(',\n",
       " 'useful',\n",
       " 'if',\n",
       " 'you',\n",
       " '’',\n",
       " 're',\n",
       " 'building',\n",
       " 'a',\n",
       " 'crawler',\n",
       " 'and',\n",
       " 'need',\n",
       " 'to',\n",
       " 'find',\n",
       " 'the',\n",
       " 'next',\n",
       " 'pages',\n",
       " 'to',\n",
       " 'visit',\n",
       " ')',\n",
       " 'Look',\n",
       " 'for',\n",
       " 'all',\n",
       " 'tags',\n",
       " 'with',\n",
       " 'a',\n",
       " 'specific',\n",
       " 'class',\n",
       " 'attribute',\n",
       " '(',\n",
       " 'eg',\n",
       " 'Look',\n",
       " 'for',\n",
       " 'the',\n",
       " 'tag',\n",
       " 'with',\n",
       " 'a',\n",
       " 'specific',\n",
       " 'ID',\n",
       " 'attribute',\n",
       " '(',\n",
       " 'eg',\n",
       " ':',\n",
       " 'Look',\n",
       " 'for',\n",
       " 'nested',\n",
       " 'patterns',\n",
       " 'of',\n",
       " 'tags',\n",
       " '(',\n",
       " 'useful',\n",
       " 'for',\n",
       " 'finding',\n",
       " 'generic',\n",
       " 'elements',\n",
       " ',',\n",
       " 'but',\n",
       " 'only',\n",
       " 'within',\n",
       " 'a',\n",
       " 'specific',\n",
       " 'section',\n",
       " 'of',\n",
       " 'the',\n",
       " 'page',\n",
       " ')',\n",
       " 'Look',\n",
       " 'for',\n",
       " 'all',\n",
       " 'tags',\n",
       " 'matching',\n",
       " 'CSS',\n",
       " 'selectors',\n",
       " '(',\n",
       " 'similar',\n",
       " 'query',\n",
       " 'to',\n",
       " 'the',\n",
       " 'last',\n",
       " 'one',\n",
       " ',',\n",
       " 'but',\n",
       " 'might',\n",
       " 'be',\n",
       " 'easier',\n",
       " 'to',\n",
       " 'write',\n",
       " 'for',\n",
       " 'someone',\n",
       " 'who',\n",
       " 'knows',\n",
       " 'CSS',\n",
       " ')',\n",
       " 'Get',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'strings',\n",
       " 'representing',\n",
       " 'the',\n",
       " 'inner',\n",
       " 'contents',\n",
       " 'of',\n",
       " 'a',\n",
       " 'tag',\n",
       " '(',\n",
       " 'this',\n",
       " 'includes',\n",
       " 'both',\n",
       " 'the',\n",
       " 'text',\n",
       " 'nodes',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'the',\n",
       " 'text',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'any',\n",
       " 'other',\n",
       " 'nested',\n",
       " 'HTML',\n",
       " 'tags',\n",
       " 'within',\n",
       " ')',\n",
       " 'Return',\n",
       " 'only',\n",
       " 'the',\n",
       " 'text',\n",
       " 'contents',\n",
       " 'within',\n",
       " 'this',\n",
       " 'tag',\n",
       " ',',\n",
       " 'but',\n",
       " 'ignore',\n",
       " 'the',\n",
       " 'text',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'other',\n",
       " 'HTML',\n",
       " 'tags',\n",
       " '(',\n",
       " 'useful',\n",
       " 'for',\n",
       " 'stripping',\n",
       " 'our',\n",
       " 'pesky',\n",
       " 'Convert',\n",
       " 'the',\n",
       " 'text',\n",
       " 'that',\n",
       " 'are',\n",
       " 'extracting',\n",
       " 'from',\n",
       " 'unicode',\n",
       " 'to',\n",
       " 'ascii',\n",
       " 'if',\n",
       " 'you',\n",
       " '’',\n",
       " 're',\n",
       " 'having',\n",
       " 'issues',\n",
       " 'printing',\n",
       " 'it',\n",
       " 'to',\n",
       " 'the',\n",
       " 'console',\n",
       " 'or',\n",
       " 'writing',\n",
       " 'it',\n",
       " 'to',\n",
       " 'files',\n",
       " 'Get',\n",
       " 'the',\n",
       " 'attribute',\n",
       " 'of',\n",
       " 'a',\n",
       " 'tag',\n",
       " '(',\n",
       " 'useful',\n",
       " 'for',\n",
       " 'grabbing',\n",
       " 'the',\n",
       " 'Putting',\n",
       " 'several',\n",
       " 'of',\n",
       " 'these',\n",
       " 'concepts',\n",
       " 'together',\n",
       " ',',\n",
       " 'here',\n",
       " '’',\n",
       " 's',\n",
       " 'a',\n",
       " 'common',\n",
       " 'idiom',\n",
       " ':',\n",
       " 'iterating',\n",
       " 'over',\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'container',\n",
       " 'tags',\n",
       " 'and',\n",
       " 'pull',\n",
       " 'out',\n",
       " 'content',\n",
       " 'from',\n",
       " 'each',\n",
       " 'of',\n",
       " 'them',\n",
       " 'BeautifulSoup',\n",
       " 'doesn',\n",
       " '’',\n",
       " 't',\n",
       " 'currently',\n",
       " 'support',\n",
       " 'XPath',\n",
       " 'selectors',\n",
       " ',',\n",
       " 'and',\n",
       " ...]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nltk.tokenize.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.probability.FreqDist(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dist.csv', 'w') as f:\n",
    "    for i in fdist:\n",
    "        f.write(f'{i},{fdist[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAETCAYAAADZHBoWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAsgUlEQVR4nO3dd3hc5Zn+8e+jXizLluQiXCTZBlOMwUiukMQOKSSwoSwlBBZIIGxCNiFhQ0iWTd0U+IUlySaBhIQESKFjwCZAKLYBgzHuxuBuuWFjyd2Sm6Tn98c5kmXjMhppmnR/rmsuzZyZ95xHbe4573vOe8zdERERAUhLdAEiIpI8FAoiItJCoSAiIi0UCiIi0kKhICIiLRQKIiLSIiPRBbRHSUmJl5eXR9V29+7d5ObmRr1ttVf7RLZPhhrUPnXbz549u9bdex32SXdP2VtlZaVHa9asWVG3VXu1T3T7ZKhB7VO3PTDLj/C+qu4jERFpoVAQEZEWCgUREWmhUBARkRYxCwUz+5OZbTKzt1stKzKzF8xsWfi1Z7jczOz/zGy5mS0wszNiVZeIiBxZLPcU7gPOOWTZt4GX3P144KXwMcCngOPD2/XA3TGsS0REjiBmoeDurwBbDll8PnB/eP9+4IJWyx8Ij5aaAfQws9JY1ba/sYm5G/fGavUiIikr3mMKfdx9Q3h/I9AnvN8PWNvqdevCZR2uscm5/J4Z/PjVrUxfXhuLTYiIpCzzGF5kx8zKgcnuPix8vM3de7R6fqu79zSzycBt7v5auPwl4BZ3n3WYdV5P0MVEaWlp5aRJk9pc16Pv7OKhRbsoyk3jF58ooVtW27Oxvr6evLy8NrdTe7XviPbJUIPap277qqqq2e5eddgnj3RWW0fcgHLg7VaPlwCl4f1SYEl4//fA5Yd73dFu0Z7RvL+h0T9++/Nedstk/+rf50S1jlQ+m1HtU799MtSg9qnbniQ6o/lp4Orw/tXAU62WXxUehTQG2O4Hupk6XEZ6Gl8bXUhuZjpPz3+Pp+atj9WmRERSSiwPSX0QeAMYambrzOxa4Dbg42a2DPhY+BjgH8BKYDnwB+CGWNXVrLRbBt8972QAvvvk27y3bXesNykikvRiNkuqu19+hKfOPsxrHfhKrGo5kstHDeCld9/npcWbuPmx+fzlC6NJS7N4lyEikjS69BnNZsZt/zqc4vwspi/fzJ9fr050SSIiCdWlQwGgV0E2P7voVABuf24xS9/fmeCKREQSp8uHAsAnTunLZVUD2NfQxNcfmse+hqZElyQikhAKhdB3/+VkBhbl8c6GHfzixaWJLkdEJCEUCqFu2RnceelppBn8btoK3qo+dIYOEZHOT6HQSlV5EV8ePxh3+MbD89i5Z3+iSxIRiSuFwiFuPPsEhvXrzrqtu/nRpHcSXY6ISFwpFA6RlZHGLy87neyMNB6dvY7n3o7ZidUiIklHoXAYQ3oX8J1PnQjAd55YyKadexJckYhIfCgUjuCqseV86PgSttbv55bHFjRP1Cci0qkpFI4gLc34+cWnUZibyZQlNfztzTWJLklEJOYUCkfRtzCHn1w4DICfPPMuK2t2JbgiEZHYUigcw3nDj+OC049j9/5GvvHIfPY36mxnEem8FAoR+OH5wziuMIf5a7fx2ynLE12OiEjMKBQiUJibyR2XngbAr19eztw1WxNckYhIbCgUIjRucAnXnVVBY5Nz0yPz2aNJ80SkE1IotME3PzmUoX0KWFVbxwMLNMW2iHQ+CoU2yMlM5xeXnU5WehrPr9jNvLXbEl2SiEiHUii00cnHdefyUQMAmLJ4U4KrERHpWAqFKJw5pASAGSs3J7gSEZGOpVCIwqiKIgyYu3Ybe/Y3JrocEZEOo1CIQo+8LMoKM9jX0KRxBRHpVBQKUTqldxagLiQR6VwUClE6pZdCQUQ6H4VClE4uCUJhzhqNK4hI56FQiFJBdhon9i1gX0MT8zWuICKdhEKhHcYMKgZgxsotCa5ERKRjKBTaYcygIkDjCiLSeSgU2mFURbCnMGfNVvY2aFxBRFKfQqEdivKzOLFvAXsbmpi/dnuiyxERaTeFQjuNrlAXkoh0HgqFdjow2KxQEJHUp1Bop1HhnoLGFUSkM1AotFNxt2xO6NONPfubWLBO4woiktoUCh2gpQtphbqQRCS1KRQ6QEsorFIoiEhqUyh0gOZxhdmrt7KvoSnB1YiIRC8hoWBm3zCzRWb2tpk9aGY5ZlZhZm+a2XIze9jMshJRWzRKumVzfO/mcYVtiS5HRCRqcQ8FM+sHfA2ocvdhQDrwWeB24BfuPgTYClwb79raQ4emikhnkKjuowwg18wygDxgA/BR4LHw+fuBCxJTWnQ0OZ6IdAZxDwV3Xw/cAawhCIPtwGxgm7s3hC9bB/SLd23toXEFEekMzN3ju0GznsDjwGXANuBRgj2EH4RdR5jZAODZsHvp0PbXA9cDlJaWVk6aNCmqOurr68nLy4uq7ZHa3/hcDet2NvKTCUWcWHL0IZFYbF/tu077ZKhB7VO3fVVV1Wx3rzrsk+4e1xtwCXBvq8dXAXcDtUBGuGws8Pyx1lVZWenRmjVrVtRtj9T+1okLvOyWyf6bl5clZPtq33XaJ0MNap+67YFZfoT31USMKawBxphZnpkZcDbwDjAFuDh8zdXAUwmorV002CwiqS4RYwpvEnQXzQEWhjXcA9wC3GRmy4Fi4N5419ZezeMKs6q3sr9R4woiknoyErFRd/8+8P1DFq8ERiWgnA7TuyCHwb3yWVFTx4J126ks65nokkRE2kRnNHcwdSGJSCpTKHSw0WEovLlK5yuISOpRKHSwMS3jCls0riAiKUeh0MF6d89hUK986vc1snC9rq8gIqlFoRADGlcQkVSlUIiB0WEX0puaB0lEUoxCIQaa9xQ0riAiqUahEAN9uucwqCSfun2NvK1xBRFJIQqFGBk9KOhC0lTaIpJKFAoxMqblfAUNNotI6lAoxMjoiiAU3lq1hQaNK4hIilAoxEjfwhwqmscV3tuR6HJERCKiUIihA4emqgtJRFKDQiGGdBKbiKQahUIMNR+B9Fb1Vo0riEhKUCjEUGlhLmXFeeza28AijSuISApQKMTYmAodmioiqUOhEGNjBuskNhFJHQqFGNP5CiKSShQKMXZcj1wGFuWxc28D72zQuIKIJDeFQhyMGaSptEUkNSgU4kDnK4hIqlAoxMHoMBRmrtpCY5MnuBoRkSNTKMRBvx65DCjKDcYVdL6CiCQxhUKc6HwFEUkFCoU40biCiKQChUKcNM+D9KbGFUQkiSkU4qR/zzz698xl554G3tX5CiKSpBQKcaQuJBFJdgqFOGq+6I7mQRKRZKVQiKMxLecrbKbRNa4gIslHoRBHA4ry6Ncjlx17GlizvSHR5YiIfECbQ8HMeprZ8FgU0xU0H4W0aNO+BFciIvJBEYWCmU01s+5mVgTMAf5gZnfGtrTOqbkL6e0ahYKIJJ9I9xQK3X0HcBHwgLuPBj4Wu7I6r7FhKLxbs48mna8gIkkmI9LXmVkpcClwawzr6fT698ylX49c1m/bzVcfnMsJfQooL8mjoiSfsuJ8CnMzE12iiHRhkYbCD4Hngdfc/S0zGwQsi11ZnZeZcfZJvXngjdU8s3ADzyzccNDzRflZlBfnUV6ST3lxPuUl+VQU51NekkdBjgJDRGIr0lDY4O4tg8vuvrI9Ywpm1gP4IzAMcOALwBLgYaAcqAYudfet0W4jmX3vvJMZkr2TrKJ+rNpcx+raeqo311G9uY4tdfvYUrePOWu2faBdcX5WS1jkNdTRb8ge+hbmxP8bEJFOK9JQ+DVwRgTLIvUr4Dl3v9jMsoA84L+Al9z9NjP7NvBt4JYo15/UMtLTOKVXFpWVAw9a3tTkvL9zD9XNIVFbF34NHm+u28fmun3MXh1k5YNvv8ynTi3lmnHlnDGwB2aWiG9HRDqRo4aCmY0FxgG9zOymVk91B9Kj2aCZFQIfBq4BcPd9wD4zOx8YH77sfmAqnTQUjiQtzSgtzKW0MJexg4sPeq45MFbVBiEx6a1lzHxvL5Pmv8ek+e8xvH8h14wr59zhpWRnRPWrERE55p5CFtAtfF1Bq+U7gIuj3GYFUAP82cxOA2YDNwJ93L25g30j0CfK9XdKrQNj3GAYmlFD6eCT+euM1Tw4cw0L1m3npkfm89N/LOaK0QO5YvRAendX15KItI15BNMtmFmZu6/ukA2aVQEzgDPd/U0z+xVByHzV3Xu0et1Wd+95mPbXA9cDlJaWVk6aNCmqOurr68nLy4uqbbK139vovLZmN88sq2d1eKZ0hsG4ATl8+vg8ji/Kiun21T7+7ZOhBrVP3fZVVVWz3b3qsE+6+zFvwAnAPcA/gZebb5G0Pcy6+gLVrR5/CHiGYKC5NFxWCiw51roqKys9WrNmzYq6bbK2b2pq8hkrav3fH5jlFd+e7GW3BLcLfvuaPzl3ne/d3xjT7at9/NonQw1qn7rtgVl+hPfVSAeaHwV+R3DEUGM0ydQqhDaa2VozG+ruS4CzgXfC29XAbeHXp9qzna7IzBg9qJjRg4pZt7Wev8xYzUMz1zJ3zTbmrpnHTwre5coxZVw+auCxVyYiXVKkodDg7nd34Ha/CvwtPPJoJfB5grOrHzGza4HVBCfKSZT698zjO586ia+ffQIT567nvtdXsfT9Xdz5wlJ+8/JyxvXP4kfl9Qwsbl8Xhoh0LpGGwiQzuwGYCOxtXujuUV0YwN3nAYfrzzo7mvXJkeVmpfO50QO5fNQA3lixmT+/Xs2L777P1NV7+Nid07j2QxV8ZcIQumVH+qcgIp1ZpO8EV4dfb261zIFBHVuOxIqZMW5ICeOGlLBmcz3ffeQNpq3ew91TV/DY7HXc/MmhXHxGf9LSdK6DSFcW0YR47l5xmJsCIUUNLM7ja6N6MPGGcYwY2IOanXv51mMLOP+303mrWleFE+nKItpTMLOrDrfc3R/o2HIknkYM7MnjXxrH0/Pf47ZnF7Nw/XYu+d0bnDe8lO98+iT69chNdIkiEmeRdh+NbHU/h6Dvfw6gUEhxaWnGBSP68YlT+vC7aSv5/bQVTF6wgRfeeZ9///AgvjR+MHlZGm8Q6Soi7T76aqvbFwnmPOoW29IknvKyMrjp4yfw8jfH8y+nHcfehib+7+XlfPSOaUycu07XfhDpIqK9RnMdwXQV0sn065HLry8fwWNfGsup/QrZuGMP33h4Phfd/Tpz13TKSWtFpJVIxxQmERxtBMFEeCcBj8SqKEm8qvIinvrKmTw+Zx3/7/klzFu7jQvvep0LR/TjlnNOTHR5IhIjkXYW39HqfgOw2t3XxaAeSSJpacYlVQP41Kml3DVlOX98bRUT567nubc3cv4JuQw/vYnM9Gh3NkUkGUU6pjANWEwwU2pPQFed70K6ZWfwrXNO5KWbPsKnhvVl9/5GHlq0i//4+xz2NzYlujwR6UARhYKZXQrMBC4hmH7iTTOLdupsSVEDivK4+8pK/nbdaPIzjecXva9gEOlkIt33vxUY6e5Xu/tVwCjgu7ErS5LZmUNK+N6Hi+iek8Hzi97nq3+fq2AQ6SQiDYU0d9/U6vHmNrSVTmhIUSZ/vW40BTkZPLdoI197UMEg0hlE+sb+nJk9b2bXmNk1BNc/+EfsypJUMLx/D/56bRAMz769kRsfUjCIpLqjhoKZDTGzM939ZuD3wPDw9gbBRXekizttQA/+cu1oCrIz+MfCjXz9oXkKBpEUdqw9hV8SXCoTd3/C3W9y95sIptD+ZWxLk1Rx+oAe/OW6IBieWbiBrz80jwYFg0hKOlYo9HH3hYcuDJeVx6QiSUmnD+jBA9eOoltzMDysYBBJRccKhR5HeU5TaMpBRgzs2RIMkxds4BuPzFcwiKSYY4XCLDP74qELzew6YHZsSpJUdsbAntz/hSAYJs1/j5sUDCIp5VjTXHwdmGhmV3AgBKqALODCGNYlKayyrCf3f2EkV907k6fnv4cZ/O8lp5GhKTFEkt5R/0vd/X13Hwf8EKgObz9097HuvjH25Umqqiwr4oFrR5Gflc5T897jPx+dT6Om3xZJepHOfTTF3X8d3l6OdVHSOVSWFXH/F1oFwyPzFAwiSU778xJTVeVF3PeFUeRlpfPkvPf4pvYYRJKaQkFibmR5sMeQl5XOxLnrufnR+TS6gkEkGSkUJC5Glhdx3+eDYHhi7nruemu79hhEkpBCQeJmVEURf75mJHlZ6UxdvYffv7Ii0SWJyCEUChJXowcV89srzgDgVy8uo7q2LsEViUhrCgWJuwlDe/ORshz2NjRx65MLcY0viCQNhYIkxDWndadnXibTl2/m8TnrE12OiIQUCpIQ3bPT+O55JwPw42feYfOuvQmuSERAoSAJdOGIfpw1pIRt9fv58TPvJrocEUGhIAlkZvzkwmFkZ6Qxce56Xllak+iSRLo8hYIkVFlxPl//2AkA3PrkQnbva0xwRSJdm0JBEu66D1VwUml31m7ZzS9fXJrockS6NIWCJFxmehq3XXQqZvDH11bx9vrtiS5JpMtSKEhSOG1AD64ZV05jk/OdJxZqCgyRBFEoSNL4z08M5bjCHBau3859r1cnuhyRLkmhIEmjW3YG/3PBMAD+959LWLe1PsEViXQ9CQsFM0s3s7lmNjl8XGFmb5rZcjN72MyyElWbJM7ZJ/Xh3OGl1O9r5HtPLdIUGCJxlsg9hRuB1mcs3Q78wt2HAFuBaxNSlSTc9//lZApyMnh58SYmL9iQ6HJEupSEhIKZ9QfOBf4YPjbgo8Bj4UvuBy5IRG2SeL0LcvivT58EwA8nLWJ7/f4EVyTSdSRqT+GXwLeApvBxMbDN3RvCx+uAfgmoS5LEZVUDGFVeRO2uffzsWU2BIRIvFu8+WzM7D/i0u99gZuOBbwLXADPCriPMbADwrLsPO0z764HrAUpLSysnTZoUVR319fXk5eVF1Vbt49N+3Y4G/vOFWhqa4Efjizil14FhplSoP5btk6EGtU/d9lVVVbPdveqwT7p7XG/Azwj2BKqBjUA98DegFsgIXzMWeP5Y66qsrPRozZo1K+q2ah+/9r98YamX3TLZJ9wxxXfva4j79pO1fTLUoPap2x6Y5Ud4X41795G7f8fd+7t7OfBZ4GV3vwKYAlwcvuxq4Kl41ybJ50vjBzGkdzdW1tRx11RdvlMk1pLpPIVbgJvMbDnBGMO9Ca5HkkB2Rjo/u+hUAO6eupyl7+9McEUinVtCQ8Hdp7r7eeH9le4+yt2HuPsl7q6rrggAI8uL+NzogexvDKbAaNIUGCIxk0x7CiJHdMs5J9K7IJvZq7fy95lrEl2OSKelUJCUUJibyQ8/cwoAtz+7mC27dd0FkVhQKEjKOGdYXz52Uh927m3g3rk7El2OSKeUkegCRCJlZvzo/FN4Y0UtM9bvZezPXqK8OJ/yknwqSvIoK86noiSfgUV55GSmJ7pckZSkUJCUclyPXH560anc/Og8Nmzfw4bte3hj5eaDXmMGxxXmUlacFwRGcT5lxXlUlOQzQIEhclQKBUk555/ej777N9Cn4iSqN9dRXVtH9eZ6VtXWsXpzHWu37mb9tuD2+orDB0ZJdiOf3LmcCUN7c2LfAoLpt0REoSApKSPNKC8Juo4YevBz+xubWLd194HAqK1j1eZ6Vm+uY11zYADzn1vC/3tuCX275zB+aC/GD+3FmUNKKMjJTMj3JJIMFArS6WSmp1FREowvHBoY+xqaWLe1nsnT57F2fwFTl9awccceHnprLQ+9tZaMNKOqvCcThvZm/NDenNCnm/YipEtRKEiXkpWRxqBe3ThzQC6Vlafh7ix6bwfTltYwdckmZq/eyoyVW5ixcgs/e3YxxxXm8JGhvZkQ7kXkZ+tfRjo3/YVLl2ZmDOtXyLB+hXxlwhC21+/n1eU1TFlcw7SlNby3fQ8PzlzDgzPXkJlujKooYvwJvend2EBloosXiQGFgkgrhXmZnDf8OM4bfhxNTcFexNQlm5i6tIa5a7Yyfflmpi8PBq8X1L3Df336JNLT1L0knYdCQeQI0tKMU/sXcmr/Qr569vFsq9/HK8tqmbp4E0/PW8+9r61izZZ6fvXZ08nL0r+SdA46o1kkQj3ysvjMacdx52Wn872PFFGYm8kL77zPZb+fwaYdexJdnkiHUCiIROGUXlk8ccM4BhblsXD9di6863WWbNS03pL6FAoiURrcqxsTbxhHZVlP1m/bzcV3v84rS2sSXZZIuygURNqhuFs2f7tuNOcOL2Xn3gY+f99bPKipvSWFKRRE2iknM51ff3YEN4wfTGNTcCGg255drIsBSUpSKIh0gLQ041vnnMjt/3oqGWnG76at4D8enMOe/brug6QWhYJIB7ps5EDu+/woCrIz+MfCjVz+hxnU7tKVZSV1KBREOthZx5fw+A3j6Ncjl7lrtnHhXdNZvklHJklqUCiIxMAJfQqY+JVxnNa/kLVbdnPRXa/z+oraRJclckwKBZEY6V2Qw0PXj+UTJ/dhx54Grv7TTB6bvS7RZYkclUJBJIZys9K5+8pKvvihCvY3Ot98dD53/nMJ7joySZKTQkEkxtLTjFvPPZn/uWAYaQb/9/JyfjVzO3sbdGSSJB+Fgkic/NuYMu69eiT5Wem8umYPV/7xTbbW7Ut0WSIHUSiIxNGEE3vzyJfGUpSbxlvVW7no7teprq1LdFkiLRQKInF2ynGF3HZ2MSeXdmdVbR0X3jWdt6q3JLosEUChIJIQxbnpPPKlsUwY2out9fu54g9v8tS89YkuS0ShIJIo3bIz+MNVVVw1tox9jU3c+NA8fvPyMh2ZJAmlUBBJoIz0NH74mVP47nknYwZ3/HMp33psAfsamhJdmnRRCgWRBDMzrj2rgt9dWUlOZhqPzl7HNX+eyfbd+xNdmnRBCgWRJPHJU/ryyL+PpVdBNq+v2My/3v06a7fUJ7os6WIUCiJJZHj/Hky8YRwn9OnG8k27uPCu6cxdszXRZUkXolAQSTL9e+bx2JfH8aHjS6jdtY/P3jODZxduSHRZ0kUoFESSUPecTP50zUg+O3IAexuauOHvc7jnlRU6MkliTqEgkqQy09P42UWncss5J+IOP/3HYm598m0aGnVkksSOQkEkiZkZXx4/mN9+7gyyMtL4+5truPb+WezcoyOTJDYy4r1BMxsAPAD0ARy4x91/ZWZFwMNAOVANXOruGmETAc4dXkrfwhy++MAspi2t4eK732BoYSPPbXwnqvWlmdGws45dBTWUF+fRr0cuGen6jCgJCAWgAfhPd59jZgXAbDN7AbgGeMndbzOzbwPfBm5JQH0iSamyrCcTbxjH5+97iyXv72TJ+8DSVe1a571zZwKQmW4M6JlHeUk+5cX5lJfkUV6cT0VJPsf1yCU9zTrgO5BUEPdQcPcNwIbw/k4zexfoB5wPjA9fdj8wFYWCyEHKivOZeMOZTJr/HktXrqZ//35RrWd/ozN36Rp2WS7VtfVs3LGHlbV1rDzMjK2Z6caAojwqivOD0CjJZ1BJPq6zrjulROwptDCzcmAE8CbQJwwMgI0E3UsicojC3EyuHFPG7MxaKisHR72e2d23U1lZCcDufY2s3lJHdW0dq2rrWb25jlW1dVRvruP9HXtZWVPHypqDAyMjDcYsfJPxQ3sxfmhvBvfKx0x7FKnOEnWIm5l1A6YBP3H3J8xsm7v3aPX8VnfveZh21wPXA5SWllZOmjQpqu3X19eTl5cXVVu1V/tEt49nDXsamti4q5ENuxrZsKuBjbsaWb2tgRVb99P63aN3XjpnlGYzom8Ww3pnkZNx9DGKRP8Mu3L7qqqq2e5eddgn3T3uNyATeB64qdWyJUBpeL8UWHKs9VRWVnq0Zs2aFXVbtVf7RLdPhhpenj7Tn5y7zm98cI6P+NE/veyWyS2342/9h1/5xxl+76srfcWmnd7U1NTh21f76NsDs/wI76uJOPrIgHuBd939zlZPPQ1cDdwWfn0q3rWJSOS6Z6cx4fR+nH96PxqbnAXrtjF1SQ1Tl2xiwfrtvLqslleX1fKjyVBWnMf4E3ox/sTejB1UTE5meqLLlyNIxJjCmcC/AQvNbF647L8IwuARM7sWWA1cmoDaRCQK6WnGiIE9GTGwJ9/4+Als3rWXV5bVMGVxDa8sq2H15nruf2M197+xmuyMNMYOLmZw7h6Ky+ooL8lPdPnSSiKOPnoNONJo1NnxrEVEYqO4WzYXjujPhSP609jkzFu7jWlLNjFlSQ0L128P9iiAe+dNpaIkn4+c0IsJJ/ZmdEWR9iISLKFHH4lI55eeZlSW9aSyrCc3fWIoNTv3Mm1pDRNnLGFhTQOraoMjne57vZqczDTGDS5h/NBeTBjamwFF7RuMl7ZTKIhIXPUqyObiyv5U8D6nnT6CeWu3MWXJJqYuqWHRezt4efEmXl68CVjEoF75TBjam/FDezGqoojsDO1FxJpCQUQSJiM9jaryIqrKi7j5kyeyaccepi4NBqtfXVYbnh+xintfW0VeVjrjBhfzkaG9+cjxvajf39SuOaDa097MOu2MtQoFEUkavbvncGnVAC6tGsD+xibmrjmwF/Huhh28+O4mXnx304EGT/6zfRtsR/u8DGPQG6+2TAdSVpxPRTg9SFF+VsqeyKdQEJGklJmexqiKIkZVFHHLOSeycfsepi3dxJTFNcys3sLuvftJT4++O6mxsTHq9g1NTdTvb+Lt9Tt4e/2ODzxfkJMRziGVT0VxHmXN90vy6ZmXmdSBoVAQkZTQtzCHy0YO5LKRAwGYPXt2yzQd0WhPe3dnyhuzKOw3mOraeqpbTQtSXVvPzj0NLFy/nYXrt3+gbfecDMpL8uluezljy1IqSoLQqCjOp2d+VtTfT0dRKIiItJGZUZidRmVZEZVlRQc95+5srttHdW0d1Zvrg/mkNgfzSlXX1rFjTwML1gVh8draZQe1LczNDGeqPTBLbbC3kU9hXmZcvjeFgohIBzIzSrplU9Itm6ryDwZG7a59VG+uY+rsRdCtF9W19ayqrWP15jq2797P/LXbmL922wfW2yMv80BQFOeTv3cv0e8nHZlCQUQkTsyMXgXZ9CrIJm1zHpWVJ7Y85+7U7Nrb0h1VHXZHNc9au61+P/PqtzEvDIwx/bK5LgY1KhRERJKAmdG7IIfeBTmMqvjgHkbNzr3hHkU9qzbXkb27NiZ1KBRERJKcmdG7ew69u+cwelAxEAyUx4IuyioiIi0UCiIi0kKhICIiLRQKIiLSQqEgIiItFAoiItJCoSAiIi0slecEN7Magus5R6MEaM/ZH2qv9olsnww1qH3qti9z916Hfcbdu+QNmKX2ap+q7ZOhBrVP7fZHuqn7SEREWigURESkRVcOhXvUXu1TuH0y1KD2qd3+sFJ6oFlERDpWV95TEBGRQygURESkhUJBRERaKBQSyMxKzSw7DtsZ3YHr6mlmo8zsw823jlr3Ubb5l/DrjbHe1lFqSDezv7VzHR/4Xcfj998RzCzNzC7tgPV84HeYyN9rPJnZJWZWEN7/bzN7wszOiLCtdcRrItpWVxpoNrM+wE+B49z9U2Z2MjDW3e9t4zpGhg9nuvumdtTzIjAYeNzdvxnB66863HJ3f+AY7V4BHLjQ3bdEU2u4nuuAG4H+wDxgDPCGu3802nWG6+3r7huP8vw7wMeAZ4HxwEF//O35ntrCzF4DPuru+6JsP8fdzzjWslgzs3FAOa2uvHisv6Gw3Sx3r2rntg/3M5jr7iMibJ8O9OHg2te0sYaD/t6O9fd3SNuzgOPd/c9m1gvo5u6rImy7wN2Hh+v4MfBz4HvufswPbWY2FXgceKr192tmWcBZwNXAFHe/L5JajqarXY7zPuDPwK3h46XAw0BEoRB+Uvo5MJXgjenXZnazuz8WTTHu/rEw3U+OsMnIVvdzgLOBOcBR/6Hd/cNm1hG/6xvDGma4+wQzO5EgZNvrXuDcozz/O+AlYBDQ+hqERhB2g462cjPbGb7usNy9e4R1rgSmm9nTQF2r9nceY/t9gX5ArpmN4ECodQfyItnwUb4HC0qI7HsI97oGE4R6Y7jYOcbfUOhFM/smwf9M6+//mKFsZpcDnwMqwp9fswIgolA3s68C3wfeB5pa1T48kvatHPr3dqy/v+btfx+oAoYSvI9kAn8Fzoxwu80/73OBe9z9GTP7cYRtzwG+ADxoZhXANoL3gHTgn8Av3X1uhOs6qq62p/CWu49s/cnEzOa5++kRtp8PfLx57yD8pPCiu58Ws6KPXk8P4CF3PydO22v++c0DRrv7XjNb5O6nxGn7dxMERHOX1SvuPr8N7f8H2AD8heDN9Aqg1N2/F2H77x9uubv/8BjtrgauIXhDmdXqqZ3Afe7+RCTb7whm9i5wskfxj29mh/tE7O5+1FAO25YBFcDPgG+3emonsMDdGyJYx3KCv7vNEZbcocK/+xHAnFbvHwvcPaJQMrPJwHrg48AZwG6C3oY2vX+YWSbBvEe73X1bW9pGoqvtKdSZWTHhJy4zGwNsb0P7tEO6izaT2HGZOoJ/tHhZFwbRk8ALZraV6CckjMZigk9mTxC8qf/FzP7g7r+OsP1nDvkHvDsM+ohCofnN38y6hY93RdjufuB+M/tXd388wlpj5W2gL0E4tom7R/235u6rCf5Wxka7DmAtbft/7Wj73N3NrPn9I7+N7S8l+MR/h7tvM7NS4Oa2FuHu+4ni9xeprhYKNwFPA4PNbDrQC7i4De2fNbPngQfDx5cB/+jYEo/MzCZxoAshHTgJeCRe23f3C8O7PzCzKUAh8Fy8tg9cC4xx9zoAM7sdeAOINBTqzOwK4CGCn+PltOoGORYzG0awl1EUPq4FrnL3RZG0d/fHzexc4BSCXf/m5T+KtIYOUAK8Y2Yzgb2tavjMsRpGO6YVtn3N3c86TDdYW7q/VgJTzewZDq79qN13HSHs5p1sZr8HepjZFwm6c/4Q6TrcvZ7gA03z4w3E8M09Wl0qFNx9jpl9hKBP0IAlYepGvArg9wQDOxCcZj6mY6s8qjta3W8AVrv7ujhuv4W7T0vAZo0D/bKE99tyxMXngF+FNwemh8sidQ9wk7tPATCz8QRvCuMiaWxmvyMYQ5gA/JHgA8nMNmy/I/ygHW2jGtMCcPezwq8F7dj+mvCWFd7iJtxDuITgg+UOgveQ77n7C/GsIx661JgCRH/kRdj2cEdORNyn2BE68uinVGNmNxEcZTExXHQBQZ/8L+O0/fmH9v8ebtlR2jcffdL8tRvwrLt/KCYFx1i8x7QSzczuB37j7m8lupZY6lJ7CtEeeWFmXwZuAAaZ2YJWTxUQfNqMi44++inVuPud4aF5zXtqn2/LERfhgQFf5IMfCr4Q4SpWmtl3CbqQAK4k6NKI1O7wa72ZHUcwJlXahvZR66Dum0PFdUwr/P19iw92v7XrkOg2GA1cYWarOfjoq7h9KIyHLhUKBEd/RHPkxd8JjpH/wJET8TpGPnQrMPLQo5+ALhEKEHQBEnRZROMp4FWCn1njMV7bwsz+4u7/FrYt50C/8CsE/cqRmhx+uv45wffgBN1IMdcR3TeJHtMC/kZwOOx5wJcI9hpr4rj9T8ZxWwnTpbqPzOxR4GvhAE/KMbOF7n5qq8dpwPzWy+TI2nL48SHtWp88N4ED50cA0Z08Z8GZzDnunsijadokHI9rFvcxLTOb7e6Vrbtsmw+TjlcNXUGX2FNo9QmngCiPvEgSCT36qROYbGafdve2/sxanzzX+jyDiE6ea+3QMS0zi3hMK9HcfdohY1rL4lxC80EhG8KjuN4jPBJMOk6X2FMIP+EYcDtBn2TLU8DtkZxmngzM7GsEx2o3D0y+6u4Tj9JEWgn70/MJPhDsp+1nA9/t7l9ux/YPO6bl7l+Ldp3xdJgxrQ8BcRvTMrPzCLrwBhAchtwd+IG7T4rH9ruKLhEKzZLh6KH2CE+J/yxBf/SfgOejOTO1KzOzIuB4Dh6ojMvhte05mzgZJPqM/vDonxubz+INf5d3tOFAAYlAl5gl1cy+bGYLgaFmtqDVbRWw4Fjtk4W7/zfBG9q9BNMmLDOzn5rZ4IQWliIsmNBvGsEJdz8Iv0Z0NnMHaT6bOFUl+oz+4a2ndQjHciKaSE8i1yXGFEieo4faLTyJZiOwkWCwryfwmJm94O7fOnrrLi9WE/odlca0OkyamfV0963QsqfQVd7D4qZL/EDDIzy2E0xrkLIsmHf+KqCW4FDGm919f3gU0jIOHi+RD9rj7nvMDDPLdvfFZjY0Dtu9gwNjWhe0Wt68LFWsI5hWpHlM6544j2n9L/BGeBQhwCXAT+K4/S6hS4RCJ1IEXBROLtbC3ZvCQTg5uoRM6Nc8ZmFmmYeOX5hZbqy334F6A1+j1ZhWPDfu7g+Y2Syg+WS1i9z9nXjW0BV0qYFmkWbhEWmFwHMe5UVz2rCtljPigRWtnioAprv7lbHcfkcKJ4b7BPB5gpNBHwHudfcVR20oKUOhIBJjZlZIMPaT8mNaAGZ2GkEonANMIZgUUmNanYRCQUQicpgxrSdbj2m5u46C6wQ0piAikdKYVhegPQUREWnRJU5eExGRyCgURESkhUJBJGRmt5rZonAKlHlmFrOJEs1sqplVxWr9ItHSQLMIYGZjCS7ecoa77zWzEuJ8HWCRZKA9BZFAKVDr7nsB3L3W3d8zs++Z2Vtm9raZ3ROevNX8Sf8XZjbLzN41s5Fm9oSZLQtns8XMys1ssZn9LXzNY2aWd+iGzewTZvaGmc0xs0fDazdjZreZ2TvhnssdcfxZSBemUBAJ/BMYYGZLzeyuVlcZ+427j3T3YUAuwd5Es33uXkVwEZ6ngK8Aw4BrzKw4fM1Q4C53PwnYQXBmc4twj+S/gY+F07rPAm4K218InBJO7f7jGHzPIh+gUBAB3H0XUAlcT3Dd34fN7Bpggpm9GU69/lGCi8Y3ezr8uhBY5O4bwj2NlQQXggFY6+7Tw/t/Bc46ZNNjgJOB6WY2j+C6w2UEEzjuAe41s4uA+o76XkWORmMKIiF3byS4qtjUMAT+HRgOVLn7WjP7Aa0uzsOB6a+bWt1vftz8v3XoiUCHPjaCKSI+MIOvmY0CzgYuBv6DAxPBicSM9hREADMbambHt1p0OrAkvF8b9vNfHMWqB4aD2ACfA1475PkZwJlmNiSsI9/MTgi3VxheT/obQFyubiaiPQWRQDfg1+HU2g3AcoKupG0EV0zbCLwVxXqXAF8xsz8B7wB3t37S3WvCbqoHzSw7XPzfwE7gKTPLIdibuCmKbYu0maa5EIkRMysHJoeD1CIpQd1HIiLSQnsKIiLSQnsKIiLSQqEgIiItFAoiItJCoSAiIi0UCiIi0kKhICIiLf4/oW+yuQtfcfwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Samples', ylabel='Counts'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.plot(20, cumulative = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'been', 'over', \"she's\", \"aren't\", 'too', 'an', 'ourselves', \"haven't\", 'if', 'her', 'haven', 'o', 'with', 'both', 'no', 're', 'how', 'mustn', \"hasn't\", 'theirs', 'them', 't', 'why', \"you're\", 'i', 'to', 'ma', \"shan't\", 's', 'up', \"it's\", 'will', 'does', 'had', \"you'll\", 'the', 'when', \"won't\", 'did', 'they', 'those', 'their', 'these', 'on', 'against', 'that', 've', 'after', 'yourselves', 'needn', 'what', 'isn', 'were', 'our', 'same', 'it', 'is', 'don', 'wouldn', 'this', 'under', 'hasn', 'between', 'or', 'by', 'you', 'because', 'here', \"should've\", 'himself', 'down', \"mightn't\", 'and', \"couldn't\", 'shan', \"needn't\", 'having', 'few', 'whom', 'she', 'won', 'through', 'aren', 'nor', 'out', 'doesn', 'more', 'yourself', 'own', 'below', 'his', 'shouldn', \"shouldn't\", 'me', 'herself', \"mustn't\", \"that'll\", 'while', 'y', 'once', 'about', 'him', 'hers', 'have', 'weren', 'wasn', 'such', 'from', 'themselves', 'ours', 'd', 'into', 'some', 'should', \"don't\", 'my', 'during', 'hadn', 'doing', 'yours', 'until', 'off', 'very', 'll', 'he', 'all', 'further', 'has', 'ain', 'as', 'in', 'itself', 'do', 'so', 'am', \"you've\", 'myself', 'for', 'not', 'any', \"doesn't\", \"wasn't\", 'just', 'again', 'now', 'we', 'but', 'than', 'each', 'can', \"isn't\", 'couldn', 'your', 'be', \"you'd\", 'then', 'there', \"didn't\", \"wouldn't\", 'who', 'mightn', 'being', 'before', 'was', 'above', \"hadn't\", 'where', 'most', 'only', 'a', 'didn', \"weren't\", 'of', 'its', 'm', 'at', 'are', 'other', 'which'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: Hartley Brody my web scraping course Useful Libraries Making Simple Requests Inspecting the Response Extracting Content from HTML Using Regular Expressions Using BeautifulSoup Using XPath Selectors Storing Your Data Writing to a CSV Writing to a SQLite Database More Advanced Topics Javascript Heavy Websites Content Inside iFrames Sessions and Cookies Delays and Backing Off Spoofing the User Agent Using Proxy Servers Setting Timeouts Handling Network Errors Learn More famously NOT recommended at all you can use the lxml library instead of BeautifulSoup, as described here. use a proxy server ebook online course free sandbox website subscribe to my blog Contact me check out my side projects web scraping content library I Don’t Need No Stinking API: Web Scraping For Fun and Profit Facebook Messenger Bot Tutorial: Step-by-Step Instructions for Building a Basic Facebook Chat Bot Web Scraping Reference: A Simple Cheat Sheet for Web Scraping with Python Startup Security Guide: Minimum Viable Security Checklist for a Cloud-Based Web Application How to Scrape Amazon.com: 19 Lessons I Learned While Crawling 1MM+ Product Listings Scaling Your Web App 101: Lessons in Architecture Under Load How HTTPS Secures Connections: What Every Web Dev Should Know Peeling Back the ORM: Demystifying Relational Databases For New Web Developers Lightning Fast Data Serialization in Python Minimum Viable Git Best Practices for Small Teams 7 Reasons I Won't Sign Your NDA Before a Coffee Meeting Focus on the Product, Not the Code: How I Build Software for Clients How I Learned to Code in Only 6 Years: And You Can Too! Web Scraping Content Library Contact Me Press Mentions Recent Projects Subscribe To My Blog Hartley Brody Once you’ve put together enough web scrapers, you start to feel like you can do it in your sleep. I’ve probably built hundreds of scrapers over the years for my own projects, as well as for clients and students in Occasionally though, I find myself referencing documentation or re-reading old code looking for snippets I can reuse. One of the students in my course suggested I put together a “cheat sheet” of commonly used code snippets and patterns for easy reference. I decided to publish it publicly as well – as an organized set of easy-to-reference notes – in case they’re helpful to others. While it’s written primarily for people who are new to programming, I also hope that it’ll be helpful to those who already have a background in software or python, but who are looking to learn some web scraping fundamentals and concepts. For the most part, a scraping program deals with making HTTP requests and parsing HTML responses. I always make sure I have Then, at the top of your Make a simple GET request (just fetching a page) Make a POST requests (usually used when sending information to the server like submitting a form) Pass query arguments aka URL parameters (usually used when making a search query or paging through results) See what response code the server sent back (useful for detecting 4XX or 5XX errors) Access the full response as text (get the HTML of the page in a big string) Look for a specific substring of text within the response Check the response’s Content Type (see if you got back HTML, JSON, XML, etc) Now that you’ve made your HTTP request and gotten some HTML content, it’s time to parse it so that you can extract the values you’re looking for. Using Regular Expressions to look for HTML patterns is However, regular expressions are still useful for finding specific string patterns like prices, email addresses or phone numbers. Run a regular expression on the response text to look for specific string patterns: BeautifulSoup is widely used due to its simple API and its powerful extraction capabilities. It has many different parser options that allow it to understand even the most poorly written HTML pages – and the default one works great. Compared to libraries that offer similar functionality, it’s a pleasure to use. To get started, you’ll have to turn the HTML text that you got in the response into a nested, DOM-like structure that you can traverse and search Look for all anchor tags on the page (useful if you’re building a crawler and need to find the next pages to visit) Look for all tags with a specific class attribute (eg Look for the tag with a specific ID attribute (eg: Look for nested patterns of tags (useful for finding generic elements, but only within a specific section of the page) Look for all tags matching CSS selectors (similar query to the last one, but might be easier to write for someone who knows CSS) Get a list of strings representing the inner contents of a tag (this includes both the text nodes as well as the text representation of any other nested HTML tags within) Return only the text contents within this tag, but ignore the text representation of other HTML tags (useful for stripping our pesky Convert the text that are extracting from unicode to ascii if you’re having issues printing it to the console or writing it to files Get the attribute of a tag (useful for grabbing the Putting several of these concepts together, here’s a common idiom: iterating over a bunch of container tags and pull out content from each of them BeautifulSoup doesn’t currently support XPath selectors, and I’ve found them to be really terse and more of a pain than they’re worth. I haven’t found a pattern I couldn’t parse using the above methods. If you’re really dedicated to using them for some reason, Now that you’ve extracted your data from the page, it’s time to save it somewhere. Note: The implication in these examples is that the scraper went out and collected all of the items, and then waited until the very end to iterate over all of them and write them to a spreadsheet or database. I did this to simplify the code examples. In practice, you’d want to store the values you extract from each page as you go, so that you don’t lose all of your progress if you hit an exception towards the end of your scrape and have to go back and re-scrape every page. Probably the most basic thing you can do is write your extracted items to a CSV file. By default, each row that is passed to the In order for the spreadsheet to make sense and have consistent columns, you need to make sure all of the items that you’ve extracted have their properties in the same order. This isn’t usually a problem if the lists are created consistently. If you’re extracting lots of properties about each item, sometimes it’s more useful to store the item as a python You can also use a simple SQL insert if you’d prefer to store your data in a database for later querying and retrieval. These aren’t really things you’ll need if you’re building a simple, small scale scraper for 90% of websites. But they’re useful tricks to keep up your sleeve. Contrary to popular belief, you do not need any special tools to scrape websites that load their content via Javascript. In order for the information to get from their server and show up on a page in your browser, that information It usually means that you won’t be making an HTTP request to the page’s URL that you see at the top of your browser window, but instead you’ll need to find the URL of the AJAX request that’s going on in the background to fetch the data from the server and load it into the page. There’s not really an easy code snippet I can show here, but if you open the Chrome or Firefox Developer Tools, you can load the page, go to the “Network” tab and then look through the all of the requests that are being sent in the background to find the one that’s returning the data you’re looking for. Start by filtering the requests to only Once you find the AJAX request that returns the data you’re hoping to scrape, then you can make your scraper send requests to this URL, instead of to the parent page’s URL. If you’re lucky, the response will be encoded with This is another topic that causes a lot of hand wringing for no reason. Sometimes the page you’re trying to scrape doesn’t actually contain the data in its HTML, but instead it loads the data inside an iframe. Again, it’s just a matter of making the request to the right URL to get the data back that you want. Make a request to the outer page, find the iframe, and then make another HTTP request to the iframe’s While HTTP is stateless, sometimes you want to use cookies to identify yourself consistently across requests to the site you’re scraping. The most common example of this is needing to login to a site in order to access protected pages. Without the correct cookies sent, a request to the URL will likely be redirected to a login form or presented with an error response. However, once you successfully login, a session cookie is set that identifies who you are to the website. As long as future requests send this cookie along, the site knows who you are and what you have access to. If you want to be polite and not overwhelm the target site you’re scraping, you can introduce an intentional delay or lag in your scraper to slow it down Some also recommend adding a backoff that’s proportional to how long the site took to respond to your request. That way if the site gets overwhelmed and starts to slow down, your code will automatically back off. By default, the More commonly, this is used to make it appear that the request is coming from a normal web browser, and not a web scraping program. Even if you spoof your User Agent, the site you are scraping can still see your IP address, since they have to know where to send the response. If you’d like to obfuscate where the request is coming from, you can If you’d like to make your requests appear to be spread out across many IP addresses, then you’ll need access to many different proxy servers. You can keep track of them in a If you’re experiencing slow connections and would prefer that your scraper moved on to something else, you can specify a timeout on your requests. Just as you should never trust user input in web applications, you shouldn’t trust the network to behave well on large web scraping projects. Eventually you’ll hit closed connections, SSL errors or other intermittent failures. If you’d like to You can also I'm a full-stack web developer and tech lead with 8 years of experience across many modern tech stacks. I'm always looking to talk to new clients and contribute to cool projects. One or two emails a month about the latest technology I'm hacking on. Check out my Web Scraping Reference: A Simple Cheat Sheet for Web Scraping with Python Useful Libraries Making Simple Requests Inspecting the Response Extracting Content from HTML Storing Your Data More Advanced Topics Learn More Table of Contents: Using Regular Expressions Using BeautifulSoup Using XPath Selectors Writing to a CSV Writing to a SQLite Database Javascript Heavy Websites Content Inside Iframes Sessions and Cookies Delays and Backing Off Spoofing the User Agent Using Proxy Servers Setting Timeouts Handling Network Errors Get Email Updates Web Scraping Guides Popular Articles More Info\n",
      "Filterd Sentence: ['Hartley', 'Brody', 'web', 'scraping', 'course', 'Useful', 'Libraries', 'Making', 'Simple', 'Requests', 'Inspecting', 'Response', 'Extracting', 'Content', 'HTML', 'Using', 'Regular', 'Expressions', 'Using', 'BeautifulSoup', 'Using', 'XPath', 'Selectors', 'Storing', 'Your', 'Data', 'Writing', 'CSV', 'Writing', 'SQLite', 'Database', 'More', 'Advanced', 'Topics', 'Javascript', 'Heavy', 'Websites', 'Content', 'Inside', 'iFrames', 'Sessions', 'Cookies', 'Delays', 'Backing', 'Off', 'Spoofing', 'User', 'Agent', 'Using', 'Proxy', 'Servers', 'Setting', 'Timeouts', 'Handling', 'Network', 'Errors', 'Learn', 'More', 'famously', 'NOT', 'recommended', 'use', 'lxml', 'library', 'instead', 'BeautifulSoup', ',', 'described', '.', 'use', 'proxy', 'server', 'ebook', 'online', 'course', 'free', 'sandbox', 'website', 'subscribe', 'blog', 'Contact', 'check', 'side', 'projects', 'web', 'scraping', 'content', 'library', 'I', 'Don', '’', 'Need', 'No', 'Stinking', 'API', ':', 'Web', 'Scraping', 'For', 'Fun', 'Profit', 'Facebook', 'Messenger', 'Bot', 'Tutorial', ':', 'Step-by-Step', 'Instructions', 'Building', 'Basic', 'Facebook', 'Chat', 'Bot', 'Web', 'Scraping', 'Reference', ':', 'A', 'Simple', 'Cheat', 'Sheet', 'Web', 'Scraping', 'Python', 'Startup', 'Security', 'Guide', ':', 'Minimum', 'Viable', 'Security', 'Checklist', 'Cloud-Based', 'Web', 'Application', 'How', 'Scrape', 'Amazon.com', ':', '19', 'Lessons', 'I', 'Learned', 'While', 'Crawling', '1MM+', 'Product', 'Listings', 'Scaling', 'Your', 'Web', 'App', '101', ':', 'Lessons', 'Architecture', 'Under', 'Load', 'How', 'HTTPS', 'Secures', 'Connections', ':', 'What', 'Every', 'Web', 'Dev', 'Should', 'Know', 'Peeling', 'Back', 'ORM', ':', 'Demystifying', 'Relational', 'Databases', 'For', 'New', 'Web', 'Developers', 'Lightning', 'Fast', 'Data', 'Serialization', 'Python', 'Minimum', 'Viable', 'Git', 'Best', 'Practices', 'Small', 'Teams', '7', 'Reasons', 'I', 'Wo', \"n't\", 'Sign', 'Your', 'NDA', 'Before', 'Coffee', 'Meeting', 'Focus', 'Product', ',', 'Not', 'Code', ':', 'How', 'I', 'Build', 'Software', 'Clients', 'How', 'I', 'Learned', 'Code', 'Only', '6', 'Years', ':', 'And', 'You', 'Can', 'Too', '!', 'Web', 'Scraping', 'Content', 'Library', 'Contact', 'Me', 'Press', 'Mentions', 'Recent', 'Projects', 'Subscribe', 'To', 'My', 'Blog', 'Hartley', 'Brody', 'Once', '’', 'put', 'together', 'enough', 'web', 'scrapers', ',', 'start', 'feel', 'like', 'sleep', '.', 'I', '’', 'probably', 'built', 'hundreds', 'scrapers', 'years', 'projects', ',', 'well', 'clients', 'students', 'Occasionally', 'though', ',', 'I', 'find', 'referencing', 'documentation', 're-reading', 'old', 'code', 'looking', 'snippets', 'I', 'reuse', '.', 'One', 'students', 'course', 'suggested', 'I', 'put', 'together', '“', 'cheat', 'sheet', '”', 'commonly', 'used', 'code', 'snippets', 'patterns', 'easy', 'reference', '.', 'I', 'decided', 'publish', 'publicly', 'well', '–', 'organized', 'set', 'easy-to-reference', 'notes', '–', 'case', '’', 'helpful', 'others', '.', 'While', '’', 'written', 'primarily', 'people', 'new', 'programming', ',', 'I', 'also', 'hope', '’', 'helpful', 'already', 'background', 'software', 'python', ',', 'looking', 'learn', 'web', 'scraping', 'fundamentals', 'concepts', '.', 'For', 'part', ',', 'scraping', 'program', 'deals', 'making', 'HTTP', 'requests', 'parsing', 'HTML', 'responses', '.', 'I', 'always', 'make', 'sure', 'I', 'Then', ',', 'top', 'Make', 'simple', 'GET', 'request', '(', 'fetching', 'page', ')', 'Make', 'POST', 'requests', '(', 'usually', 'used', 'sending', 'information', 'server', 'like', 'submitting', 'form', ')', 'Pass', 'query', 'arguments', 'aka', 'URL', 'parameters', '(', 'usually', 'used', 'making', 'search', 'query', 'paging', 'results', ')', 'See', 'response', 'code', 'server', 'sent', 'back', '(', 'useful', 'detecting', '4XX', '5XX', 'errors', ')', 'Access', 'full', 'response', 'text', '(', 'get', 'HTML', 'page', 'big', 'string', ')', 'Look', 'specific', 'substring', 'text', 'within', 'response', 'Check', 'response', '’', 'Content', 'Type', '(', 'see', 'got', 'back', 'HTML', ',', 'JSON', ',', 'XML', ',', 'etc', ')', 'Now', '’', 'made', 'HTTP', 'request', 'gotten', 'HTML', 'content', ',', '’', 'time', 'parse', 'extract', 'values', '’', 'looking', '.', 'Using', 'Regular', 'Expressions', 'look', 'HTML', 'patterns', 'However', ',', 'regular', 'expressions', 'still', 'useful', 'finding', 'specific', 'string', 'patterns', 'like', 'prices', ',', 'email', 'addresses', 'phone', 'numbers', '.', 'Run', 'regular', 'expression', 'response', 'text', 'look', 'specific', 'string', 'patterns', ':', 'BeautifulSoup', 'widely', 'used', 'due', 'simple', 'API', 'powerful', 'extraction', 'capabilities', '.', 'It', 'many', 'different', 'parser', 'options', 'allow', 'understand', 'even', 'poorly', 'written', 'HTML', 'pages', '–', 'default', 'one', 'works', 'great', '.', 'Compared', 'libraries', 'offer', 'similar', 'functionality', ',', '’', 'pleasure', 'use', '.', 'To', 'get', 'started', ',', '’', 'turn', 'HTML', 'text', 'got', 'response', 'nested', ',', 'DOM-like', 'structure', 'traverse', 'search', 'Look', 'anchor', 'tags', 'page', '(', 'useful', '’', 'building', 'crawler', 'need', 'find', 'next', 'pages', 'visit', ')', 'Look', 'tags', 'specific', 'class', 'attribute', '(', 'eg', 'Look', 'tag', 'specific', 'ID', 'attribute', '(', 'eg', ':', 'Look', 'nested', 'patterns', 'tags', '(', 'useful', 'finding', 'generic', 'elements', ',', 'within', 'specific', 'section', 'page', ')', 'Look', 'tags', 'matching', 'CSS', 'selectors', '(', 'similar', 'query', 'last', 'one', ',', 'might', 'easier', 'write', 'someone', 'knows', 'CSS', ')', 'Get', 'list', 'strings', 'representing', 'inner', 'contents', 'tag', '(', 'includes', 'text', 'nodes', 'well', 'text', 'representation', 'nested', 'HTML', 'tags', 'within', ')', 'Return', 'text', 'contents', 'within', 'tag', ',', 'ignore', 'text', 'representation', 'HTML', 'tags', '(', 'useful', 'stripping', 'pesky', 'Convert', 'text', 'extracting', 'unicode', 'ascii', '’', 'issues', 'printing', 'console', 'writing', 'files', 'Get', 'attribute', 'tag', '(', 'useful', 'grabbing', 'Putting', 'several', 'concepts', 'together', ',', '’', 'common', 'idiom', ':', 'iterating', 'bunch', 'container', 'tags', 'pull', 'content', 'BeautifulSoup', '’', 'currently', 'support', 'XPath', 'selectors', ',', 'I', '’', 'found', 'really', 'terse', 'pain', '’', 'worth', '.', 'I', '’', 'found', 'pattern', 'I', '’', 'parse', 'using', 'methods', '.', 'If', '’', 'really', 'dedicated', 'using', 'reason', ',', 'Now', '’', 'extracted', 'data', 'page', ',', '’', 'time', 'save', 'somewhere', '.', 'Note', ':', 'The', 'implication', 'examples', 'scraper', 'went', 'collected', 'items', ',', 'waited', 'end', 'iterate', 'write', 'spreadsheet', 'database', '.', 'I', 'simplify', 'code', 'examples', '.', 'In', 'practice', ',', '’', 'want', 'store', 'values', 'extract', 'page', 'go', ',', '’', 'lose', 'progress', 'hit', 'exception', 'towards', 'end', 'scrape', 'go', 'back', 're-scrape', 'every', 'page', '.', 'Probably', 'basic', 'thing', 'write', 'extracted', 'items', 'CSV', 'file', '.', 'By', 'default', ',', 'row', 'passed', 'In', 'order', 'spreadsheet', 'make', 'sense', 'consistent', 'columns', ',', 'need', 'make', 'sure', 'items', '’', 'extracted', 'properties', 'order', '.', 'This', '’', 'usually', 'problem', 'lists', 'created', 'consistently', '.', 'If', '’', 'extracting', 'lots', 'properties', 'item', ',', 'sometimes', '’', 'useful', 'store', 'item', 'python', 'You', 'also', 'use', 'simple', 'SQL', 'insert', '’', 'prefer', 'store', 'data', 'database', 'later', 'querying', 'retrieval', '.', 'These', '’', 'really', 'things', '’', 'need', '’', 'building', 'simple', ',', 'small', 'scale', 'scraper', '90', '%', 'websites', '.', 'But', '’', 'useful', 'tricks', 'keep', 'sleeve', '.', 'Contrary', 'popular', 'belief', ',', 'need', 'special', 'tools', 'scrape', 'websites', 'load', 'content', 'via', 'Javascript', '.', 'In', 'order', 'information', 'get', 'server', 'show', 'page', 'browser', ',', 'information', 'It', 'usually', 'means', '’', 'making', 'HTTP', 'request', 'page', '’', 'URL', 'see', 'top', 'browser', 'window', ',', 'instead', '’', 'need', 'find', 'URL', 'AJAX', 'request', '’', 'going', 'background', 'fetch', 'data', 'server', 'load', 'page', '.', 'There', '’', 'really', 'easy', 'code', 'snippet', 'I', 'show', ',', 'open', 'Chrome', 'Firefox', 'Developer', 'Tools', ',', 'load', 'page', ',', 'go', '“', 'Network', '”', 'tab', 'look', 'requests', 'sent', 'background', 'find', 'one', '’', 'returning', 'data', '’', 'looking', '.', 'Start', 'filtering', 'requests', 'Once', 'find', 'AJAX', 'request', 'returns', 'data', '’', 'hoping', 'scrape', ',', 'make', 'scraper', 'send', 'requests', 'URL', ',', 'instead', 'parent', 'page', '’', 'URL', '.', 'If', '’', 'lucky', ',', 'response', 'encoded', 'This', 'another', 'topic', 'causes', 'lot', 'hand', 'wringing', 'reason', '.', 'Sometimes', 'page', '’', 'trying', 'scrape', '’', 'actually', 'contain', 'data', 'HTML', ',', 'instead', 'loads', 'data', 'inside', 'iframe', '.', 'Again', ',', '’', 'matter', 'making', 'request', 'right', 'URL', 'get', 'data', 'back', 'want', '.', 'Make', 'request', 'outer', 'page', ',', 'find', 'iframe', ',', 'make', 'another', 'HTTP', 'request', 'iframe', '’', 'While', 'HTTP', 'stateless', ',', 'sometimes', 'want', 'use', 'cookies', 'identify', 'consistently', 'across', 'requests', 'site', '’', 'scraping', '.', 'The', 'common', 'example', 'needing', 'login', 'site', 'order', 'access', 'protected', 'pages', '.', 'Without', 'correct', 'cookies', 'sent', ',', 'request', 'URL', 'likely', 'redirected', 'login', 'form', 'presented', 'error', 'response', '.', 'However', ',', 'successfully', 'login', ',', 'session', 'cookie', 'set', 'identifies', 'website', '.', 'As', 'long', 'future', 'requests', 'send', 'cookie', 'along', ',', 'site', 'knows', 'access', '.', 'If', 'want', 'polite', 'overwhelm', 'target', 'site', '’', 'scraping', ',', 'introduce', 'intentional', 'delay', 'lag', 'scraper', 'slow', 'Some', 'also', 'recommend', 'adding', 'backoff', '’', 'proportional', 'long', 'site', 'took', 'respond', 'request', '.', 'That', 'way', 'site', 'gets', 'overwhelmed', 'starts', 'slow', ',', 'code', 'automatically', 'back', '.', 'By', 'default', ',', 'More', 'commonly', ',', 'used', 'make', 'appear', 'request', 'coming', 'normal', 'web', 'browser', ',', 'web', 'scraping', 'program', '.', 'Even', 'spoof', 'User', 'Agent', ',', 'site', 'scraping', 'still', 'see', 'IP', 'address', ',', 'since', 'know', 'send', 'response', '.', 'If', '’', 'like', 'obfuscate', 'request', 'coming', ',', 'If', '’', 'like', 'make', 'requests', 'appear', 'spread', 'across', 'many', 'IP', 'addresses', ',', '’', 'need', 'access', 'many', 'different', 'proxy', 'servers', '.', 'You', 'keep', 'track', 'If', '’', 'experiencing', 'slow', 'connections', 'would', 'prefer', 'scraper', 'moved', 'something', 'else', ',', 'specify', 'timeout', 'requests', '.', 'Just', 'never', 'trust', 'user', 'input', 'web', 'applications', ',', '’', 'trust', 'network', 'behave', 'well', 'large', 'web', 'scraping', 'projects', '.', 'Eventually', '’', 'hit', 'closed', 'connections', ',', 'SSL', 'errors', 'intermittent', 'failures', '.', 'If', '’', 'like', 'You', 'also', 'I', \"'m\", 'full-stack', 'web', 'developer', 'tech', 'lead', '8', 'years', 'experience', 'across', 'many', 'modern', 'tech', 'stacks', '.', 'I', \"'m\", 'always', 'looking', 'talk', 'new', 'clients', 'contribute', 'cool', 'projects', '.', 'One', 'two', 'emails', 'month', 'latest', 'technology', 'I', \"'m\", 'hacking', '.', 'Check', 'Web', 'Scraping', 'Reference', ':', 'A', 'Simple', 'Cheat', 'Sheet', 'Web', 'Scraping', 'Python', 'Useful', 'Libraries', 'Making', 'Simple', 'Requests', 'Inspecting', 'Response', 'Extracting', 'Content', 'HTML', 'Storing', 'Your', 'Data', 'More', 'Advanced', 'Topics', 'Learn', 'More', 'Table', 'Contents', ':', 'Using', 'Regular', 'Expressions', 'Using', 'BeautifulSoup', 'Using', 'XPath', 'Selectors', 'Writing', 'CSV', 'Writing', 'SQLite', 'Database', 'Javascript', 'Heavy', 'Websites', 'Content', 'Inside', 'Iframes', 'Sessions', 'Cookies', 'Delays', 'Backing', 'Off', 'Spoofing', 'User', 'Agent', 'Using', 'Proxy', 'Servers', 'Setting', 'Timeouts', 'Handling', 'Network', 'Errors', 'Get', 'Email', 'Updates', 'Web', 'Scraping', 'Guides', 'Popular', 'Articles', 'More', 'Info']\n"
     ]
    }
   ],
   "source": [
    "filtered_sent=[]\n",
    "for w in token:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "print(\"Tokenized Sentence:\",text)\n",
    "print(\"Filterd Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['Hartley', 'Brody', 'web', 'scraping', 'course', 'Useful', 'Libraries', 'Making', 'Simple', 'Requests', 'Inspecting', 'Response', 'Extracting', 'Content', 'HTML', 'Using', 'Regular', 'Expressions', 'Using', 'BeautifulSoup', 'Using', 'XPath', 'Selectors', 'Storing', 'Your', 'Data', 'Writing', 'CSV', 'Writing', 'SQLite', 'Database', 'More', 'Advanced', 'Topics', 'Javascript', 'Heavy', 'Websites', 'Content', 'Inside', 'iFrames', 'Sessions', 'Cookies', 'Delays', 'Backing', 'Off', 'Spoofing', 'User', 'Agent', 'Using', 'Proxy', 'Servers', 'Setting', 'Timeouts', 'Handling', 'Network', 'Errors', 'Learn', 'More', 'famously', 'NOT', 'recommended', 'use', 'lxml', 'library', 'instead', 'BeautifulSoup', ',', 'described', '.', 'use', 'proxy', 'server', 'ebook', 'online', 'course', 'free', 'sandbox', 'website', 'subscribe', 'blog', 'Contact', 'check', 'side', 'projects', 'web', 'scraping', 'content', 'library', 'I', 'Don', '’', 'Need', 'No', 'Stinking', 'API', ':', 'Web', 'Scraping', 'For', 'Fun', 'Profit', 'Facebook', 'Messenger', 'Bot', 'Tutorial', ':', 'Step-by-Step', 'Instructions', 'Building', 'Basic', 'Facebook', 'Chat', 'Bot', 'Web', 'Scraping', 'Reference', ':', 'A', 'Simple', 'Cheat', 'Sheet', 'Web', 'Scraping', 'Python', 'Startup', 'Security', 'Guide', ':', 'Minimum', 'Viable', 'Security', 'Checklist', 'Cloud-Based', 'Web', 'Application', 'How', 'Scrape', 'Amazon.com', ':', '19', 'Lessons', 'I', 'Learned', 'While', 'Crawling', '1MM+', 'Product', 'Listings', 'Scaling', 'Your', 'Web', 'App', '101', ':', 'Lessons', 'Architecture', 'Under', 'Load', 'How', 'HTTPS', 'Secures', 'Connections', ':', 'What', 'Every', 'Web', 'Dev', 'Should', 'Know', 'Peeling', 'Back', 'ORM', ':', 'Demystifying', 'Relational', 'Databases', 'For', 'New', 'Web', 'Developers', 'Lightning', 'Fast', 'Data', 'Serialization', 'Python', 'Minimum', 'Viable', 'Git', 'Best', 'Practices', 'Small', 'Teams', '7', 'Reasons', 'I', 'Wo', \"n't\", 'Sign', 'Your', 'NDA', 'Before', 'Coffee', 'Meeting', 'Focus', 'Product', ',', 'Not', 'Code', ':', 'How', 'I', 'Build', 'Software', 'Clients', 'How', 'I', 'Learned', 'Code', 'Only', '6', 'Years', ':', 'And', 'You', 'Can', 'Too', '!', 'Web', 'Scraping', 'Content', 'Library', 'Contact', 'Me', 'Press', 'Mentions', 'Recent', 'Projects', 'Subscribe', 'To', 'My', 'Blog', 'Hartley', 'Brody', 'Once', '’', 'put', 'together', 'enough', 'web', 'scrapers', ',', 'start', 'feel', 'like', 'sleep', '.', 'I', '’', 'probably', 'built', 'hundreds', 'scrapers', 'years', 'projects', ',', 'well', 'clients', 'students', 'Occasionally', 'though', ',', 'I', 'find', 'referencing', 'documentation', 're-reading', 'old', 'code', 'looking', 'snippets', 'I', 'reuse', '.', 'One', 'students', 'course', 'suggested', 'I', 'put', 'together', '“', 'cheat', 'sheet', '”', 'commonly', 'used', 'code', 'snippets', 'patterns', 'easy', 'reference', '.', 'I', 'decided', 'publish', 'publicly', 'well', '–', 'organized', 'set', 'easy-to-reference', 'notes', '–', 'case', '’', 'helpful', 'others', '.', 'While', '’', 'written', 'primarily', 'people', 'new', 'programming', ',', 'I', 'also', 'hope', '’', 'helpful', 'already', 'background', 'software', 'python', ',', 'looking', 'learn', 'web', 'scraping', 'fundamentals', 'concepts', '.', 'For', 'part', ',', 'scraping', 'program', 'deals', 'making', 'HTTP', 'requests', 'parsing', 'HTML', 'responses', '.', 'I', 'always', 'make', 'sure', 'I', 'Then', ',', 'top', 'Make', 'simple', 'GET', 'request', '(', 'fetching', 'page', ')', 'Make', 'POST', 'requests', '(', 'usually', 'used', 'sending', 'information', 'server', 'like', 'submitting', 'form', ')', 'Pass', 'query', 'arguments', 'aka', 'URL', 'parameters', '(', 'usually', 'used', 'making', 'search', 'query', 'paging', 'results', ')', 'See', 'response', 'code', 'server', 'sent', 'back', '(', 'useful', 'detecting', '4XX', '5XX', 'errors', ')', 'Access', 'full', 'response', 'text', '(', 'get', 'HTML', 'page', 'big', 'string', ')', 'Look', 'specific', 'substring', 'text', 'within', 'response', 'Check', 'response', '’', 'Content', 'Type', '(', 'see', 'got', 'back', 'HTML', ',', 'JSON', ',', 'XML', ',', 'etc', ')', 'Now', '’', 'made', 'HTTP', 'request', 'gotten', 'HTML', 'content', ',', '’', 'time', 'parse', 'extract', 'values', '’', 'looking', '.', 'Using', 'Regular', 'Expressions', 'look', 'HTML', 'patterns', 'However', ',', 'regular', 'expressions', 'still', 'useful', 'finding', 'specific', 'string', 'patterns', 'like', 'prices', ',', 'email', 'addresses', 'phone', 'numbers', '.', 'Run', 'regular', 'expression', 'response', 'text', 'look', 'specific', 'string', 'patterns', ':', 'BeautifulSoup', 'widely', 'used', 'due', 'simple', 'API', 'powerful', 'extraction', 'capabilities', '.', 'It', 'many', 'different', 'parser', 'options', 'allow', 'understand', 'even', 'poorly', 'written', 'HTML', 'pages', '–', 'default', 'one', 'works', 'great', '.', 'Compared', 'libraries', 'offer', 'similar', 'functionality', ',', '’', 'pleasure', 'use', '.', 'To', 'get', 'started', ',', '’', 'turn', 'HTML', 'text', 'got', 'response', 'nested', ',', 'DOM-like', 'structure', 'traverse', 'search', 'Look', 'anchor', 'tags', 'page', '(', 'useful', '’', 'building', 'crawler', 'need', 'find', 'next', 'pages', 'visit', ')', 'Look', 'tags', 'specific', 'class', 'attribute', '(', 'eg', 'Look', 'tag', 'specific', 'ID', 'attribute', '(', 'eg', ':', 'Look', 'nested', 'patterns', 'tags', '(', 'useful', 'finding', 'generic', 'elements', ',', 'within', 'specific', 'section', 'page', ')', 'Look', 'tags', 'matching', 'CSS', 'selectors', '(', 'similar', 'query', 'last', 'one', ',', 'might', 'easier', 'write', 'someone', 'knows', 'CSS', ')', 'Get', 'list', 'strings', 'representing', 'inner', 'contents', 'tag', '(', 'includes', 'text', 'nodes', 'well', 'text', 'representation', 'nested', 'HTML', 'tags', 'within', ')', 'Return', 'text', 'contents', 'within', 'tag', ',', 'ignore', 'text', 'representation', 'HTML', 'tags', '(', 'useful', 'stripping', 'pesky', 'Convert', 'text', 'extracting', 'unicode', 'ascii', '’', 'issues', 'printing', 'console', 'writing', 'files', 'Get', 'attribute', 'tag', '(', 'useful', 'grabbing', 'Putting', 'several', 'concepts', 'together', ',', '’', 'common', 'idiom', ':', 'iterating', 'bunch', 'container', 'tags', 'pull', 'content', 'BeautifulSoup', '’', 'currently', 'support', 'XPath', 'selectors', ',', 'I', '’', 'found', 'really', 'terse', 'pain', '’', 'worth', '.', 'I', '’', 'found', 'pattern', 'I', '’', 'parse', 'using', 'methods', '.', 'If', '’', 'really', 'dedicated', 'using', 'reason', ',', 'Now', '’', 'extracted', 'data', 'page', ',', '’', 'time', 'save', 'somewhere', '.', 'Note', ':', 'The', 'implication', 'examples', 'scraper', 'went', 'collected', 'items', ',', 'waited', 'end', 'iterate', 'write', 'spreadsheet', 'database', '.', 'I', 'simplify', 'code', 'examples', '.', 'In', 'practice', ',', '’', 'want', 'store', 'values', 'extract', 'page', 'go', ',', '’', 'lose', 'progress', 'hit', 'exception', 'towards', 'end', 'scrape', 'go', 'back', 're-scrape', 'every', 'page', '.', 'Probably', 'basic', 'thing', 'write', 'extracted', 'items', 'CSV', 'file', '.', 'By', 'default', ',', 'row', 'passed', 'In', 'order', 'spreadsheet', 'make', 'sense', 'consistent', 'columns', ',', 'need', 'make', 'sure', 'items', '’', 'extracted', 'properties', 'order', '.', 'This', '’', 'usually', 'problem', 'lists', 'created', 'consistently', '.', 'If', '’', 'extracting', 'lots', 'properties', 'item', ',', 'sometimes', '’', 'useful', 'store', 'item', 'python', 'You', 'also', 'use', 'simple', 'SQL', 'insert', '’', 'prefer', 'store', 'data', 'database', 'later', 'querying', 'retrieval', '.', 'These', '’', 'really', 'things', '’', 'need', '’', 'building', 'simple', ',', 'small', 'scale', 'scraper', '90', '%', 'websites', '.', 'But', '’', 'useful', 'tricks', 'keep', 'sleeve', '.', 'Contrary', 'popular', 'belief', ',', 'need', 'special', 'tools', 'scrape', 'websites', 'load', 'content', 'via', 'Javascript', '.', 'In', 'order', 'information', 'get', 'server', 'show', 'page', 'browser', ',', 'information', 'It', 'usually', 'means', '’', 'making', 'HTTP', 'request', 'page', '’', 'URL', 'see', 'top', 'browser', 'window', ',', 'instead', '’', 'need', 'find', 'URL', 'AJAX', 'request', '’', 'going', 'background', 'fetch', 'data', 'server', 'load', 'page', '.', 'There', '’', 'really', 'easy', 'code', 'snippet', 'I', 'show', ',', 'open', 'Chrome', 'Firefox', 'Developer', 'Tools', ',', 'load', 'page', ',', 'go', '“', 'Network', '”', 'tab', 'look', 'requests', 'sent', 'background', 'find', 'one', '’', 'returning', 'data', '’', 'looking', '.', 'Start', 'filtering', 'requests', 'Once', 'find', 'AJAX', 'request', 'returns', 'data', '’', 'hoping', 'scrape', ',', 'make', 'scraper', 'send', 'requests', 'URL', ',', 'instead', 'parent', 'page', '’', 'URL', '.', 'If', '’', 'lucky', ',', 'response', 'encoded', 'This', 'another', 'topic', 'causes', 'lot', 'hand', 'wringing', 'reason', '.', 'Sometimes', 'page', '’', 'trying', 'scrape', '’', 'actually', 'contain', 'data', 'HTML', ',', 'instead', 'loads', 'data', 'inside', 'iframe', '.', 'Again', ',', '’', 'matter', 'making', 'request', 'right', 'URL', 'get', 'data', 'back', 'want', '.', 'Make', 'request', 'outer', 'page', ',', 'find', 'iframe', ',', 'make', 'another', 'HTTP', 'request', 'iframe', '’', 'While', 'HTTP', 'stateless', ',', 'sometimes', 'want', 'use', 'cookies', 'identify', 'consistently', 'across', 'requests', 'site', '’', 'scraping', '.', 'The', 'common', 'example', 'needing', 'login', 'site', 'order', 'access', 'protected', 'pages', '.', 'Without', 'correct', 'cookies', 'sent', ',', 'request', 'URL', 'likely', 'redirected', 'login', 'form', 'presented', 'error', 'response', '.', 'However', ',', 'successfully', 'login', ',', 'session', 'cookie', 'set', 'identifies', 'website', '.', 'As', 'long', 'future', 'requests', 'send', 'cookie', 'along', ',', 'site', 'knows', 'access', '.', 'If', 'want', 'polite', 'overwhelm', 'target', 'site', '’', 'scraping', ',', 'introduce', 'intentional', 'delay', 'lag', 'scraper', 'slow', 'Some', 'also', 'recommend', 'adding', 'backoff', '’', 'proportional', 'long', 'site', 'took', 'respond', 'request', '.', 'That', 'way', 'site', 'gets', 'overwhelmed', 'starts', 'slow', ',', 'code', 'automatically', 'back', '.', 'By', 'default', ',', 'More', 'commonly', ',', 'used', 'make', 'appear', 'request', 'coming', 'normal', 'web', 'browser', ',', 'web', 'scraping', 'program', '.', 'Even', 'spoof', 'User', 'Agent', ',', 'site', 'scraping', 'still', 'see', 'IP', 'address', ',', 'since', 'know', 'send', 'response', '.', 'If', '’', 'like', 'obfuscate', 'request', 'coming', ',', 'If', '’', 'like', 'make', 'requests', 'appear', 'spread', 'across', 'many', 'IP', 'addresses', ',', '’', 'need', 'access', 'many', 'different', 'proxy', 'servers', '.', 'You', 'keep', 'track', 'If', '’', 'experiencing', 'slow', 'connections', 'would', 'prefer', 'scraper', 'moved', 'something', 'else', ',', 'specify', 'timeout', 'requests', '.', 'Just', 'never', 'trust', 'user', 'input', 'web', 'applications', ',', '’', 'trust', 'network', 'behave', 'well', 'large', 'web', 'scraping', 'projects', '.', 'Eventually', '’', 'hit', 'closed', 'connections', ',', 'SSL', 'errors', 'intermittent', 'failures', '.', 'If', '’', 'like', 'You', 'also', 'I', \"'m\", 'full-stack', 'web', 'developer', 'tech', 'lead', '8', 'years', 'experience', 'across', 'many', 'modern', 'tech', 'stacks', '.', 'I', \"'m\", 'always', 'looking', 'talk', 'new', 'clients', 'contribute', 'cool', 'projects', '.', 'One', 'two', 'emails', 'month', 'latest', 'technology', 'I', \"'m\", 'hacking', '.', 'Check', 'Web', 'Scraping', 'Reference', ':', 'A', 'Simple', 'Cheat', 'Sheet', 'Web', 'Scraping', 'Python', 'Useful', 'Libraries', 'Making', 'Simple', 'Requests', 'Inspecting', 'Response', 'Extracting', 'Content', 'HTML', 'Storing', 'Your', 'Data', 'More', 'Advanced', 'Topics', 'Learn', 'More', 'Table', 'Contents', ':', 'Using', 'Regular', 'Expressions', 'Using', 'BeautifulSoup', 'Using', 'XPath', 'Selectors', 'Writing', 'CSV', 'Writing', 'SQLite', 'Database', 'Javascript', 'Heavy', 'Websites', 'Content', 'Inside', 'Iframes', 'Sessions', 'Cookies', 'Delays', 'Backing', 'Off', 'Spoofing', 'User', 'Agent', 'Using', 'Proxy', 'Servers', 'Setting', 'Timeouts', 'Handling', 'Network', 'Errors', 'Get', 'Email', 'Updates', 'Web', 'Scraping', 'Guides', 'Popular', 'Articles', 'More', 'Info']\n",
      "Stemmed Sentence: ['hartley', 'brodi', 'web', 'scrape', 'cours', 'use', 'librari', 'make', 'simpl', 'request', 'inspect', 'respons', 'extract', 'content', 'html', 'use', 'regular', 'express', 'use', 'beautifulsoup', 'use', 'xpath', 'selector', 'store', 'your', 'data', 'write', 'csv', 'write', 'sqlite', 'databas', 'more', 'advanc', 'topic', 'javascript', 'heavi', 'websit', 'content', 'insid', 'ifram', 'session', 'cooki', 'delay', 'back', 'off', 'spoof', 'user', 'agent', 'use', 'proxi', 'server', 'set', 'timeout', 'handl', 'network', 'error', 'learn', 'more', 'famous', 'not', 'recommend', 'use', 'lxml', 'librari', 'instead', 'beautifulsoup', ',', 'describ', '.', 'use', 'proxi', 'server', 'ebook', 'onlin', 'cours', 'free', 'sandbox', 'websit', 'subscrib', 'blog', 'contact', 'check', 'side', 'project', 'web', 'scrape', 'content', 'librari', 'I', 'don', '’', 'need', 'No', 'stink', 'api', ':', 'web', 'scrape', 'for', 'fun', 'profit', 'facebook', 'messeng', 'bot', 'tutori', ':', 'step-by-step', 'instruct', 'build', 'basic', 'facebook', 'chat', 'bot', 'web', 'scrape', 'refer', ':', 'A', 'simpl', 'cheat', 'sheet', 'web', 'scrape', 'python', 'startup', 'secur', 'guid', ':', 'minimum', 'viabl', 'secur', 'checklist', 'cloud-bas', 'web', 'applic', 'how', 'scrape', 'amazon.com', ':', '19', 'lesson', 'I', 'learn', 'while', 'crawl', '1mm+', 'product', 'list', 'scale', 'your', 'web', 'app', '101', ':', 'lesson', 'architectur', 'under', 'load', 'how', 'http', 'secur', 'connect', ':', 'what', 'everi', 'web', 'dev', 'should', 'know', 'peel', 'back', 'orm', ':', 'demystifi', 'relat', 'databas', 'for', 'new', 'web', 'develop', 'lightn', 'fast', 'data', 'serial', 'python', 'minimum', 'viabl', 'git', 'best', 'practic', 'small', 'team', '7', 'reason', 'I', 'Wo', \"n't\", 'sign', 'your', 'nda', 'befor', 'coffe', 'meet', 'focu', 'product', ',', 'not', 'code', ':', 'how', 'I', 'build', 'softwar', 'client', 'how', 'I', 'learn', 'code', 'onli', '6', 'year', ':', 'and', 'you', 'can', 'too', '!', 'web', 'scrape', 'content', 'librari', 'contact', 'Me', 'press', 'mention', 'recent', 'project', 'subscrib', 'To', 'My', 'blog', 'hartley', 'brodi', 'onc', '’', 'put', 'togeth', 'enough', 'web', 'scraper', ',', 'start', 'feel', 'like', 'sleep', '.', 'I', '’', 'probabl', 'built', 'hundr', 'scraper', 'year', 'project', ',', 'well', 'client', 'student', 'occasion', 'though', ',', 'I', 'find', 'referenc', 'document', 're-read', 'old', 'code', 'look', 'snippet', 'I', 'reus', '.', 'one', 'student', 'cours', 'suggest', 'I', 'put', 'togeth', '“', 'cheat', 'sheet', '”', 'commonli', 'use', 'code', 'snippet', 'pattern', 'easi', 'refer', '.', 'I', 'decid', 'publish', 'publicli', 'well', '–', 'organ', 'set', 'easy-to-refer', 'note', '–', 'case', '’', 'help', 'other', '.', 'while', '’', 'written', 'primarili', 'peopl', 'new', 'program', ',', 'I', 'also', 'hope', '’', 'help', 'alreadi', 'background', 'softwar', 'python', ',', 'look', 'learn', 'web', 'scrape', 'fundament', 'concept', '.', 'for', 'part', ',', 'scrape', 'program', 'deal', 'make', 'http', 'request', 'pars', 'html', 'respons', '.', 'I', 'alway', 'make', 'sure', 'I', 'then', ',', 'top', 'make', 'simpl', 'get', 'request', '(', 'fetch', 'page', ')', 'make', 'post', 'request', '(', 'usual', 'use', 'send', 'inform', 'server', 'like', 'submit', 'form', ')', 'pass', 'queri', 'argument', 'aka', 'url', 'paramet', '(', 'usual', 'use', 'make', 'search', 'queri', 'page', 'result', ')', 'see', 'respons', 'code', 'server', 'sent', 'back', '(', 'use', 'detect', '4xx', '5xx', 'error', ')', 'access', 'full', 'respons', 'text', '(', 'get', 'html', 'page', 'big', 'string', ')', 'look', 'specif', 'substr', 'text', 'within', 'respons', 'check', 'respons', '’', 'content', 'type', '(', 'see', 'got', 'back', 'html', ',', 'json', ',', 'xml', ',', 'etc', ')', 'now', '’', 'made', 'http', 'request', 'gotten', 'html', 'content', ',', '’', 'time', 'pars', 'extract', 'valu', '’', 'look', '.', 'use', 'regular', 'express', 'look', 'html', 'pattern', 'howev', ',', 'regular', 'express', 'still', 'use', 'find', 'specif', 'string', 'pattern', 'like', 'price', ',', 'email', 'address', 'phone', 'number', '.', 'run', 'regular', 'express', 'respons', 'text', 'look', 'specif', 'string', 'pattern', ':', 'beautifulsoup', 'wide', 'use', 'due', 'simpl', 'api', 'power', 'extract', 'capabl', '.', 'It', 'mani', 'differ', 'parser', 'option', 'allow', 'understand', 'even', 'poorli', 'written', 'html', 'page', '–', 'default', 'one', 'work', 'great', '.', 'compar', 'librari', 'offer', 'similar', 'function', ',', '’', 'pleasur', 'use', '.', 'To', 'get', 'start', ',', '’', 'turn', 'html', 'text', 'got', 'respons', 'nest', ',', 'dom-lik', 'structur', 'travers', 'search', 'look', 'anchor', 'tag', 'page', '(', 'use', '’', 'build', 'crawler', 'need', 'find', 'next', 'page', 'visit', ')', 'look', 'tag', 'specif', 'class', 'attribut', '(', 'eg', 'look', 'tag', 'specif', 'ID', 'attribut', '(', 'eg', ':', 'look', 'nest', 'pattern', 'tag', '(', 'use', 'find', 'gener', 'element', ',', 'within', 'specif', 'section', 'page', ')', 'look', 'tag', 'match', 'css', 'selector', '(', 'similar', 'queri', 'last', 'one', ',', 'might', 'easier', 'write', 'someon', 'know', 'css', ')', 'get', 'list', 'string', 'repres', 'inner', 'content', 'tag', '(', 'includ', 'text', 'node', 'well', 'text', 'represent', 'nest', 'html', 'tag', 'within', ')', 'return', 'text', 'content', 'within', 'tag', ',', 'ignor', 'text', 'represent', 'html', 'tag', '(', 'use', 'strip', 'peski', 'convert', 'text', 'extract', 'unicod', 'ascii', '’', 'issu', 'print', 'consol', 'write', 'file', 'get', 'attribut', 'tag', '(', 'use', 'grab', 'put', 'sever', 'concept', 'togeth', ',', '’', 'common', 'idiom', ':', 'iter', 'bunch', 'contain', 'tag', 'pull', 'content', 'beautifulsoup', '’', 'current', 'support', 'xpath', 'selector', ',', 'I', '’', 'found', 'realli', 'ters', 'pain', '’', 'worth', '.', 'I', '’', 'found', 'pattern', 'I', '’', 'pars', 'use', 'method', '.', 'If', '’', 'realli', 'dedic', 'use', 'reason', ',', 'now', '’', 'extract', 'data', 'page', ',', '’', 'time', 'save', 'somewher', '.', 'note', ':', 'the', 'implic', 'exampl', 'scraper', 'went', 'collect', 'item', ',', 'wait', 'end', 'iter', 'write', 'spreadsheet', 'databas', '.', 'I', 'simplifi', 'code', 'exampl', '.', 'In', 'practic', ',', '’', 'want', 'store', 'valu', 'extract', 'page', 'go', ',', '’', 'lose', 'progress', 'hit', 'except', 'toward', 'end', 'scrape', 'go', 'back', 're-scrap', 'everi', 'page', '.', 'probabl', 'basic', 'thing', 'write', 'extract', 'item', 'csv', 'file', '.', 'By', 'default', ',', 'row', 'pass', 'In', 'order', 'spreadsheet', 'make', 'sens', 'consist', 'column', ',', 'need', 'make', 'sure', 'item', '’', 'extract', 'properti', 'order', '.', 'thi', '’', 'usual', 'problem', 'list', 'creat', 'consist', '.', 'If', '’', 'extract', 'lot', 'properti', 'item', ',', 'sometim', '’', 'use', 'store', 'item', 'python', 'you', 'also', 'use', 'simpl', 'sql', 'insert', '’', 'prefer', 'store', 'data', 'databas', 'later', 'queri', 'retriev', '.', 'these', '’', 'realli', 'thing', '’', 'need', '’', 'build', 'simpl', ',', 'small', 'scale', 'scraper', '90', '%', 'websit', '.', 'but', '’', 'use', 'trick', 'keep', 'sleev', '.', 'contrari', 'popular', 'belief', ',', 'need', 'special', 'tool', 'scrape', 'websit', 'load', 'content', 'via', 'javascript', '.', 'In', 'order', 'inform', 'get', 'server', 'show', 'page', 'browser', ',', 'inform', 'It', 'usual', 'mean', '’', 'make', 'http', 'request', 'page', '’', 'url', 'see', 'top', 'browser', 'window', ',', 'instead', '’', 'need', 'find', 'url', 'ajax', 'request', '’', 'go', 'background', 'fetch', 'data', 'server', 'load', 'page', '.', 'there', '’', 'realli', 'easi', 'code', 'snippet', 'I', 'show', ',', 'open', 'chrome', 'firefox', 'develop', 'tool', ',', 'load', 'page', ',', 'go', '“', 'network', '”', 'tab', 'look', 'request', 'sent', 'background', 'find', 'one', '’', 'return', 'data', '’', 'look', '.', 'start', 'filter', 'request', 'onc', 'find', 'ajax', 'request', 'return', 'data', '’', 'hope', 'scrape', ',', 'make', 'scraper', 'send', 'request', 'url', ',', 'instead', 'parent', 'page', '’', 'url', '.', 'If', '’', 'lucki', ',', 'respons', 'encod', 'thi', 'anoth', 'topic', 'caus', 'lot', 'hand', 'wring', 'reason', '.', 'sometim', 'page', '’', 'tri', 'scrape', '’', 'actual', 'contain', 'data', 'html', ',', 'instead', 'load', 'data', 'insid', 'ifram', '.', 'again', ',', '’', 'matter', 'make', 'request', 'right', 'url', 'get', 'data', 'back', 'want', '.', 'make', 'request', 'outer', 'page', ',', 'find', 'ifram', ',', 'make', 'anoth', 'http', 'request', 'ifram', '’', 'while', 'http', 'stateless', ',', 'sometim', 'want', 'use', 'cooki', 'identifi', 'consist', 'across', 'request', 'site', '’', 'scrape', '.', 'the', 'common', 'exampl', 'need', 'login', 'site', 'order', 'access', 'protect', 'page', '.', 'without', 'correct', 'cooki', 'sent', ',', 'request', 'url', 'like', 'redirect', 'login', 'form', 'present', 'error', 'respons', '.', 'howev', ',', 'success', 'login', ',', 'session', 'cooki', 'set', 'identifi', 'websit', '.', 'As', 'long', 'futur', 'request', 'send', 'cooki', 'along', ',', 'site', 'know', 'access', '.', 'If', 'want', 'polit', 'overwhelm', 'target', 'site', '’', 'scrape', ',', 'introduc', 'intent', 'delay', 'lag', 'scraper', 'slow', 'some', 'also', 'recommend', 'ad', 'backoff', '’', 'proport', 'long', 'site', 'took', 'respond', 'request', '.', 'that', 'way', 'site', 'get', 'overwhelm', 'start', 'slow', ',', 'code', 'automat', 'back', '.', 'By', 'default', ',', 'more', 'commonli', ',', 'use', 'make', 'appear', 'request', 'come', 'normal', 'web', 'browser', ',', 'web', 'scrape', 'program', '.', 'even', 'spoof', 'user', 'agent', ',', 'site', 'scrape', 'still', 'see', 'IP', 'address', ',', 'sinc', 'know', 'send', 'respons', '.', 'If', '’', 'like', 'obfusc', 'request', 'come', ',', 'If', '’', 'like', 'make', 'request', 'appear', 'spread', 'across', 'mani', 'IP', 'address', ',', '’', 'need', 'access', 'mani', 'differ', 'proxi', 'server', '.', 'you', 'keep', 'track', 'If', '’', 'experienc', 'slow', 'connect', 'would', 'prefer', 'scraper', 'move', 'someth', 'els', ',', 'specifi', 'timeout', 'request', '.', 'just', 'never', 'trust', 'user', 'input', 'web', 'applic', ',', '’', 'trust', 'network', 'behav', 'well', 'larg', 'web', 'scrape', 'project', '.', 'eventu', '’', 'hit', 'close', 'connect', ',', 'ssl', 'error', 'intermitt', 'failur', '.', 'If', '’', 'like', 'you', 'also', 'I', \"'m\", 'full-stack', 'web', 'develop', 'tech', 'lead', '8', 'year', 'experi', 'across', 'mani', 'modern', 'tech', 'stack', '.', 'I', \"'m\", 'alway', 'look', 'talk', 'new', 'client', 'contribut', 'cool', 'project', '.', 'one', 'two', 'email', 'month', 'latest', 'technolog', 'I', \"'m\", 'hack', '.', 'check', 'web', 'scrape', 'refer', ':', 'A', 'simpl', 'cheat', 'sheet', 'web', 'scrape', 'python', 'use', 'librari', 'make', 'simpl', 'request', 'inspect', 'respons', 'extract', 'content', 'html', 'store', 'your', 'data', 'more', 'advanc', 'topic', 'learn', 'more', 'tabl', 'content', ':', 'use', 'regular', 'express', 'use', 'beautifulsoup', 'use', 'xpath', 'selector', 'write', 'csv', 'write', 'sqlite', 'databas', 'javascript', 'heavi', 'websit', 'content', 'insid', 'ifram', 'session', 'cooki', 'delay', 'back', 'off', 'spoof', 'user', 'agent', 'use', 'proxi', 'server', 'set', 'timeout', 'handl', 'network', 'error', 'get', 'email', 'updat', 'web', 'scrape', 'guid', 'popular', 'articl', 'more', 'info']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Stemmed Sentence:\",stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: Hartley Brody my web scraping course Useful Libraries Making Simple Requests Inspecting the Response Extracting Content from HTML Using Regular Expressions Using BeautifulSoup Using XPath Selectors Storing Your Data Writing to a CSV Writing to a SQLite Database More Advanced Topics Javascript Heavy Websites Content Inside iFrames Sessions and Cookies Delays and Backing Off Spoofing the User Agent Using Proxy Servers Setting Timeouts Handling Network Errors Learn More famously NOT recommended at all you can use the lxml library instead of BeautifulSoup, as described here. use a proxy server ebook online course free sandbox website subscribe to my blog Contact me check out my side projects web scraping content library I Don’t Need No Stinking API: Web Scraping For Fun and Profit Facebook Messenger Bot Tutorial: Step-by-Step Instructions for Building a Basic Facebook Chat Bot Web Scraping Reference: A Simple Cheat Sheet for Web Scraping with Python Startup Security Guide: Minimum Viable Security Checklist for a Cloud-Based Web Application How to Scrape Amazon.com: 19 Lessons I Learned While Crawling 1MM+ Product Listings Scaling Your Web App 101: Lessons in Architecture Under Load How HTTPS Secures Connections: What Every Web Dev Should Know Peeling Back the ORM: Demystifying Relational Databases For New Web Developers Lightning Fast Data Serialization in Python Minimum Viable Git Best Practices for Small Teams 7 Reasons I Won't Sign Your NDA Before a Coffee Meeting Focus on the Product, Not the Code: How I Build Software for Clients How I Learned to Code in Only 6 Years: And You Can Too! Web Scraping Content Library Contact Me Press Mentions Recent Projects Subscribe To My Blog Hartley Brody Once you’ve put together enough web scrapers, you start to feel like you can do it in your sleep. I’ve probably built hundreds of scrapers over the years for my own projects, as well as for clients and students in Occasionally though, I find myself referencing documentation or re-reading old code looking for snippets I can reuse. One of the students in my course suggested I put together a “cheat sheet” of commonly used code snippets and patterns for easy reference. I decided to publish it publicly as well – as an organized set of easy-to-reference notes – in case they’re helpful to others. While it’s written primarily for people who are new to programming, I also hope that it’ll be helpful to those who already have a background in software or python, but who are looking to learn some web scraping fundamentals and concepts. For the most part, a scraping program deals with making HTTP requests and parsing HTML responses. I always make sure I have Then, at the top of your Make a simple GET request (just fetching a page) Make a POST requests (usually used when sending information to the server like submitting a form) Pass query arguments aka URL parameters (usually used when making a search query or paging through results) See what response code the server sent back (useful for detecting 4XX or 5XX errors) Access the full response as text (get the HTML of the page in a big string) Look for a specific substring of text within the response Check the response’s Content Type (see if you got back HTML, JSON, XML, etc) Now that you’ve made your HTTP request and gotten some HTML content, it’s time to parse it so that you can extract the values you’re looking for. Using Regular Expressions to look for HTML patterns is However, regular expressions are still useful for finding specific string patterns like prices, email addresses or phone numbers. Run a regular expression on the response text to look for specific string patterns: BeautifulSoup is widely used due to its simple API and its powerful extraction capabilities. It has many different parser options that allow it to understand even the most poorly written HTML pages – and the default one works great. Compared to libraries that offer similar functionality, it’s a pleasure to use. To get started, you’ll have to turn the HTML text that you got in the response into a nested, DOM-like structure that you can traverse and search Look for all anchor tags on the page (useful if you’re building a crawler and need to find the next pages to visit) Look for all tags with a specific class attribute (eg Look for the tag with a specific ID attribute (eg: Look for nested patterns of tags (useful for finding generic elements, but only within a specific section of the page) Look for all tags matching CSS selectors (similar query to the last one, but might be easier to write for someone who knows CSS) Get a list of strings representing the inner contents of a tag (this includes both the text nodes as well as the text representation of any other nested HTML tags within) Return only the text contents within this tag, but ignore the text representation of other HTML tags (useful for stripping our pesky Convert the text that are extracting from unicode to ascii if you’re having issues printing it to the console or writing it to files Get the attribute of a tag (useful for grabbing the Putting several of these concepts together, here’s a common idiom: iterating over a bunch of container tags and pull out content from each of them BeautifulSoup doesn’t currently support XPath selectors, and I’ve found them to be really terse and more of a pain than they’re worth. I haven’t found a pattern I couldn’t parse using the above methods. If you’re really dedicated to using them for some reason, Now that you’ve extracted your data from the page, it’s time to save it somewhere. Note: The implication in these examples is that the scraper went out and collected all of the items, and then waited until the very end to iterate over all of them and write them to a spreadsheet or database. I did this to simplify the code examples. In practice, you’d want to store the values you extract from each page as you go, so that you don’t lose all of your progress if you hit an exception towards the end of your scrape and have to go back and re-scrape every page. Probably the most basic thing you can do is write your extracted items to a CSV file. By default, each row that is passed to the In order for the spreadsheet to make sense and have consistent columns, you need to make sure all of the items that you’ve extracted have their properties in the same order. This isn’t usually a problem if the lists are created consistently. If you’re extracting lots of properties about each item, sometimes it’s more useful to store the item as a python You can also use a simple SQL insert if you’d prefer to store your data in a database for later querying and retrieval. These aren’t really things you’ll need if you’re building a simple, small scale scraper for 90% of websites. But they’re useful tricks to keep up your sleeve. Contrary to popular belief, you do not need any special tools to scrape websites that load their content via Javascript. In order for the information to get from their server and show up on a page in your browser, that information It usually means that you won’t be making an HTTP request to the page’s URL that you see at the top of your browser window, but instead you’ll need to find the URL of the AJAX request that’s going on in the background to fetch the data from the server and load it into the page. There’s not really an easy code snippet I can show here, but if you open the Chrome or Firefox Developer Tools, you can load the page, go to the “Network” tab and then look through the all of the requests that are being sent in the background to find the one that’s returning the data you’re looking for. Start by filtering the requests to only Once you find the AJAX request that returns the data you’re hoping to scrape, then you can make your scraper send requests to this URL, instead of to the parent page’s URL. If you’re lucky, the response will be encoded with This is another topic that causes a lot of hand wringing for no reason. Sometimes the page you’re trying to scrape doesn’t actually contain the data in its HTML, but instead it loads the data inside an iframe. Again, it’s just a matter of making the request to the right URL to get the data back that you want. Make a request to the outer page, find the iframe, and then make another HTTP request to the iframe’s While HTTP is stateless, sometimes you want to use cookies to identify yourself consistently across requests to the site you’re scraping. The most common example of this is needing to login to a site in order to access protected pages. Without the correct cookies sent, a request to the URL will likely be redirected to a login form or presented with an error response. However, once you successfully login, a session cookie is set that identifies who you are to the website. As long as future requests send this cookie along, the site knows who you are and what you have access to. If you want to be polite and not overwhelm the target site you’re scraping, you can introduce an intentional delay or lag in your scraper to slow it down Some also recommend adding a backoff that’s proportional to how long the site took to respond to your request. That way if the site gets overwhelmed and starts to slow down, your code will automatically back off. By default, the More commonly, this is used to make it appear that the request is coming from a normal web browser, and not a web scraping program. Even if you spoof your User Agent, the site you are scraping can still see your IP address, since they have to know where to send the response. If you’d like to obfuscate where the request is coming from, you can If you’d like to make your requests appear to be spread out across many IP addresses, then you’ll need access to many different proxy servers. You can keep track of them in a If you’re experiencing slow connections and would prefer that your scraper moved on to something else, you can specify a timeout on your requests. Just as you should never trust user input in web applications, you shouldn’t trust the network to behave well on large web scraping projects. Eventually you’ll hit closed connections, SSL errors or other intermittent failures. If you’d like to You can also I'm a full-stack web developer and tech lead with 8 years of experience across many modern tech stacks. I'm always looking to talk to new clients and contribute to cool projects. One or two emails a month about the latest technology I'm hacking on. Check out my Web Scraping Reference: A Simple Cheat Sheet for Web Scraping with Python Useful Libraries Making Simple Requests Inspecting the Response Extracting Content from HTML Storing Your Data More Advanced Topics Learn More Table of Contents: Using Regular Expressions Using BeautifulSoup Using XPath Selectors Writing to a CSV Writing to a SQLite Database Javascript Heavy Websites Content Inside Iframes Sessions and Cookies Delays and Backing Off Spoofing the User Agent Using Proxy Servers Setting Timeouts Handling Network Errors Get Email Updates Web Scraping Guides Popular Articles More Info\n",
      "Stemmed Word: hartley brody my web scraping course useful libraries making simple requests inspecting the response extracting content from html using regular expressions using beautifulsoup using xpath selectors storing your data writing to a csv writing to a sqlite database more advanced topics javascript heavy websites content inside iframes sessions and cookies delays and backing off spoofing the user agent using proxy servers setting timeouts handling network errors learn more famously not recommended at all you can use the lxml library instead of beautifulsoup, as described here. use a proxy server ebook online course free sandbox website subscribe to my blog contact me check out my side projects web scraping content library i don’t need no stinking api: web scraping for fun and profit facebook messenger bot tutorial: step-by-step instructions for building a basic facebook chat bot web scraping reference: a simple cheat sheet for web scraping with python startup security guide: minimum viable security checklist for a cloud-based web application how to scrape amazon.com: 19 lessons i learned while crawling 1mm+ product listings scaling your web app 101: lessons in architecture under load how https secures connections: what every web dev should know peeling back the orm: demystifying relational databases for new web developers lightning fast data serialization in python minimum viable git best practices for small teams 7 reasons i won't sign your nda before a coffee meeting focus on the product, not the code: how i build software for clients how i learned to code in only 6 years: and you can too! web scraping content library contact me press mentions recent projects subscribe to my blog hartley brody once you’ve put together enough web scrapers, you start to feel like you can do it in your sleep. i’ve probably built hundreds of scrapers over the years for my own projects, as well as for clients and students in occasionally though, i find myself referencing documentation or re-reading old code looking for snippets i can reuse. one of the students in my course suggested i put together a “cheat sheet” of commonly used code snippets and patterns for easy reference. i decided to publish it publicly as well – as an organized set of easy-to-reference notes – in case they’re helpful to others. while it’s written primarily for people who are new to programming, i also hope that it’ll be helpful to those who already have a background in software or python, but who are looking to learn some web scraping fundamentals and concepts. for the most part, a scraping program deals with making http requests and parsing html responses. i always make sure i have then, at the top of your make a simple get request (just fetching a page) make a post requests (usually used when sending information to the server like submitting a form) pass query arguments aka url parameters (usually used when making a search query or paging through results) see what response code the server sent back (useful for detecting 4xx or 5xx errors) access the full response as text (get the html of the page in a big string) look for a specific substring of text within the response check the response’s content type (see if you got back html, json, xml, etc) now that you’ve made your http request and gotten some html content, it’s time to parse it so that you can extract the values you’re looking for. using regular expressions to look for html patterns is however, regular expressions are still useful for finding specific string patterns like prices, email addresses or phone numbers. run a regular expression on the response text to look for specific string patterns: beautifulsoup is widely used due to its simple api and its powerful extraction capabilities. it has many different parser options that allow it to understand even the most poorly written html pages – and the default one works great. compared to libraries that offer similar functionality, it’s a pleasure to use. to get started, you’ll have to turn the html text that you got in the response into a nested, dom-like structure that you can traverse and search look for all anchor tags on the page (useful if you’re building a crawler and need to find the next pages to visit) look for all tags with a specific class attribute (eg look for the tag with a specific id attribute (eg: look for nested patterns of tags (useful for finding generic elements, but only within a specific section of the page) look for all tags matching css selectors (similar query to the last one, but might be easier to write for someone who knows css) get a list of strings representing the inner contents of a tag (this includes both the text nodes as well as the text representation of any other nested html tags within) return only the text contents within this tag, but ignore the text representation of other html tags (useful for stripping our pesky convert the text that are extracting from unicode to ascii if you’re having issues printing it to the console or writing it to files get the attribute of a tag (useful for grabbing the putting several of these concepts together, here’s a common idiom: iterating over a bunch of container tags and pull out content from each of them beautifulsoup doesn’t currently support xpath selectors, and i’ve found them to be really terse and more of a pain than they’re worth. i haven’t found a pattern i couldn’t parse using the above methods. if you’re really dedicated to using them for some reason, now that you’ve extracted your data from the page, it’s time to save it somewhere. note: the implication in these examples is that the scraper went out and collected all of the items, and then waited until the very end to iterate over all of them and write them to a spreadsheet or database. i did this to simplify the code examples. in practice, you’d want to store the values you extract from each page as you go, so that you don’t lose all of your progress if you hit an exception towards the end of your scrape and have to go back and re-scrape every page. probably the most basic thing you can do is write your extracted items to a csv file. by default, each row that is passed to the in order for the spreadsheet to make sense and have consistent columns, you need to make sure all of the items that you’ve extracted have their properties in the same order. this isn’t usually a problem if the lists are created consistently. if you’re extracting lots of properties about each item, sometimes it’s more useful to store the item as a python you can also use a simple sql insert if you’d prefer to store your data in a database for later querying and retrieval. these aren’t really things you’ll need if you’re building a simple, small scale scraper for 90% of websites. but they’re useful tricks to keep up your sleeve. contrary to popular belief, you do not need any special tools to scrape websites that load their content via javascript. in order for the information to get from their server and show up on a page in your browser, that information it usually means that you won’t be making an http request to the page’s url that you see at the top of your browser window, but instead you’ll need to find the url of the ajax request that’s going on in the background to fetch the data from the server and load it into the page. there’s not really an easy code snippet i can show here, but if you open the chrome or firefox developer tools, you can load the page, go to the “network” tab and then look through the all of the requests that are being sent in the background to find the one that’s returning the data you’re looking for. start by filtering the requests to only once you find the ajax request that returns the data you’re hoping to scrape, then you can make your scraper send requests to this url, instead of to the parent page’s url. if you’re lucky, the response will be encoded with this is another topic that causes a lot of hand wringing for no reason. sometimes the page you’re trying to scrape doesn’t actually contain the data in its html, but instead it loads the data inside an iframe. again, it’s just a matter of making the request to the right url to get the data back that you want. make a request to the outer page, find the iframe, and then make another http request to the iframe’s while http is stateless, sometimes you want to use cookies to identify yourself consistently across requests to the site you’re scraping. the most common example of this is needing to login to a site in order to access protected pages. without the correct cookies sent, a request to the url will likely be redirected to a login form or presented with an error response. however, once you successfully login, a session cookie is set that identifies who you are to the website. as long as future requests send this cookie along, the site knows who you are and what you have access to. if you want to be polite and not overwhelm the target site you’re scraping, you can introduce an intentional delay or lag in your scraper to slow it down some also recommend adding a backoff that’s proportional to how long the site took to respond to your request. that way if the site gets overwhelmed and starts to slow down, your code will automatically back off. by default, the more commonly, this is used to make it appear that the request is coming from a normal web browser, and not a web scraping program. even if you spoof your user agent, the site you are scraping can still see your ip address, since they have to know where to send the response. if you’d like to obfuscate where the request is coming from, you can if you’d like to make your requests appear to be spread out across many ip addresses, then you’ll need access to many different proxy servers. you can keep track of them in a if you’re experiencing slow connections and would prefer that your scraper moved on to something else, you can specify a timeout on your requests. just as you should never trust user input in web applications, you shouldn’t trust the network to behave well on large web scraping projects. eventually you’ll hit closed connections, ssl errors or other intermittent failures. if you’d like to you can also i'm a full-stack web developer and tech lead with 8 years of experience across many modern tech stacks. i'm always looking to talk to new clients and contribute to cool projects. one or two emails a month about the latest technology i'm hacking on. check out my web scraping reference: a simple cheat sheet for web scraping with python useful libraries making simple requests inspecting the response extracting content from html storing your data more advanced topics learn more table of contents: using regular expressions using beautifulsoup using xpath selectors writing to a csv writing to a sqlite database javascript heavy websites content inside iframes sessions and cookies delays and backing off spoofing the user agent using proxy servers setting timeouts handling network errors get email updates web scraping guides popular articles more info\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"flying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(text,\"v\"))\n",
    "print(\"Stemmed Word:\",stem.stem(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlrd\n",
      "  Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
      "Installing collected packages: xlrd\n",
      "Successfully installed xlrd-1.2.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/Users/matheus/Documents/MBA FGV/Análise de Mídias Sociais e Mineração de Textos/python-text-mining/venv-text/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.read_excel(\n",
    "    '/Volumes/GoogleDrive/Meu Drive/FIP VBC/1. Ativo/Portifólio - Gestão/5. iMedicina/1. Financeiro/Demonstrações Financeiras iMedicina V07.xlsx',\n",
    "    skiprows=6,\n",
    "    sheet_name = '1.2018'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                        ATIVO\n",
       "1                             ATIVO CIRCULANTE\n",
       "2                                   DISPONÍVEL\n",
       "3                                        CAIXA\n",
       "4                                        CAIXA\n",
       "5                        CAIXA BRADESCO CARTÃO\n",
       "6                       BANCOS CONTA MOVIMENTO\n",
       "7                  BANCO BRADESCO C/C 355089-3\n",
       "8     APLICAÇÕES FINANCEIRAS LIQUIDEZ IMEDIATA\n",
       "9     APLICAÇAO BANCO DO BRADESCO C/C 355089-3\n",
       "10            TITULO DE CAPITALIZAÇAO BRADESCO\n",
       "11                                    CLIENTES\n",
       "12                        DUPLICATAS A RECEBER\n",
       "13                           CLIENTES DIVERSOS\n",
       "14                             OUTROS CRÉDITOS\n",
       "15                        DESPESAS ANTECIPADAS\n",
       "16        DESP. ANTECIPADA EMPRESTIMO BRADESCO\n",
       "17                ADIANTAMENTOS A FORNECEDORES\n",
       "18                   ADIANTAMENTO FORNECEDORES\n",
       "19              TRIBUTOS A RECUPERAR/COMPENSAR\n",
       "Name: Descrição da conta, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p['Descrição da conta'].fillna(p['Unnamed: 8']).fillna(p['Unnamed: 9']).fillna(p['Unnamed: 10']).fillna(p['Unnamed: 11']).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
